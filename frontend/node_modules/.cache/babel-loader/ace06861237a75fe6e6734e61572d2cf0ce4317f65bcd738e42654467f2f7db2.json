{"ast":null,"code":"var _jsxFileName = \"/home/user/Documents/Aruco_POC/aruco-detector/src/CameraFeed.js\",\n  _s = $RefreshSig$();\n// import React, { useEffect, useRef, useState } from 'react';\n// import axios from 'axios';\n\n// const CameraFeed = () => {\n//   const [detected, setDetected] = useState(false);\n//   const [completed, setCompleted] = useState(false);\n//   const [currentStep, setCurrentStep] = useState(0);  // Track the current step\n//   const [isTutorialStarted, setIsTutorialStarted] = useState(false); // Track whether tutorial has started\n//   const [isAudioPlaying, setIsAudioPlaying] = useState(false);\n//   const videoRef = useRef(null);\n//   const canvasRef = useRef(null);\n//   const audioRefs = useRef([\n//     new Audio('/voice1.mp3'), // Step 1: Introduction to sulfate test\n//     new Audio('/voice2.mp3'), // Step 2: Show distilled water\n//     new Audio('/voice3.mp3'), // Step 3: Show test tube 1\n//     new Audio('/voice4.mp3'),\n//     new Audio('/voice5.mp3'),\n//     new Audio('/voice6.mp3'),\n//     new Audio('/voice7.mp3'),\n//     new Audio('/voice8.mp3'),\n//     new Audio('/voice9.mp3'),\n//     new Audio('/voice10.mp3'),\n//     new Audio('/voice11.mp3'),\n//     new Audio('/voice12.mp3'),\n//     new Audio('/voice13.mp3'), // Step 13: Test completion message\n//   ]);\n\n//   // Start camera feed and process frames only when the tutorial is started\n//   useEffect(() => {\n//     if (isTutorialStarted) {\n//       const video = videoRef.current;\n\n//       // Web camera initialization logic\n//       navigator.mediaDevices.getUserMedia({ video: true })\n//         .then((stream) => {\n//           video.srcObject = stream;\n//           video.onloadedmetadata = () => {\n//             video.play();\n//           };\n//         })\n//         .catch(err => console.error(\"Error accessing webcam:\", err));\n\n//       // Clean up: stop the camera stream when component unmounts\n//       return () => {\n//         if (video.srcObject) {\n//           video.srcObject.getTracks().forEach(track => track.stop());\n//         }\n//       };\n//     }\n//   }, [isTutorialStarted]);  // Only run this effect when the tutorial starts\n\n//   const processFrame = async () => {\n//     if (!isTutorialStarted || completed) return;  // Stop processing if the tutorial hasn't started or test is complete\n\n//     const canvas = canvasRef.current;\n//     const context = canvas.getContext('2d');\n\n//     // Ensure video is ready and draw the video feed onto the canvas\n//     const video = videoRef.current;\n//     if (video.readyState === video.HAVE_ENOUGH_DATA) {\n//       context.drawImage(video, 0, 0, canvas.width, canvas.height);\n\n//       // Send frame to backend for ArUco detection\n//       canvas.toBlob(async (blob) => {\n//         const formData = new FormData();\n//         formData.append('image', blob, 'frame.jpg');\n//         try {\n//           const response = await axios.post('http://localhost:5000/detect', formData, { responseType: 'json' });\n//           // console.log(\"Response from backend:\", response.data);  // Log response\n\n//           const { detected, augmented_image, ids, currentStep } = response.data;  // Destructure response\n\n//           if (ids) {\n//               handleMarkerDetection(ids, currentStep);\n//             }\n\n//           // Clear the canvas before drawing the new image\n//           context.clearRect(0, 0, canvas.width, canvas.height);\n\n//           // Display processed frame (augmented) coming from backend\n//           const img = new Image();\n//           img.src = `data:image/jpeg;base64,${augmented_image}`;\n//           img.onload = () => {\n//             context.drawImage(img, 0, 0, canvas.width, canvas.height);\n//           };\n//         } catch (err) {\n//           console.error(\"Error processing frame:\", err);\n//         }\n//       }, 'image/jpeg');\n//     }\n//   };\n\n//   const handleMarkerDetection = (ids,currentStepFromBackend) => {\n//     if (completed) return;\n//     // console.log('inside handleMarkerDetection');\n//      // Stop if completed\n\n//         // For step 5, ensure both IDs 1 and 3 are present\n//     if (currentStep === 5 && ids.includes(1) && ids.includes(3)) {\n//       console.log('Step 5: Detected both Marker 1 and Marker 3');\n//       setCurrentStep(currentStep + 1);\n//       playNextAudio();\n//       return;\n//     }\n\n//     if (currentStep === 6 && ids.includes(0) && ids.includes(1)) {\n//       console.log('Step 6: Detected both Marker 0 and Marker 1');\n//       setCurrentStep(currentStep + 1);\n//       playNextAudio();\n//       return;\n//     }\n\n//     // Step 7: Detect marker ID=1 with alternating y-axis conditions\n//     if (currentStep === 7 && ids.includes(1) && !ids.includes(0)) {\n//       console.log('Step 7: Marker 1 detected, alternating y-axis required');\n//       // Assume the backend sends this information via the currentStepFromBackend\n//       setCurrentStep(currentStep + 1);\n//       playNextAudio();\n//       return;\n//     }\n\n//     // Step 8: Detect marker ID=1 with alternating y-axis conditions\n//     if (currentStep === 8 && ids.includes(1) && ids.includes(2)) {\n//       console.log('Step 8: Detected both Marker 1 and Marker 2');\n//       // Assume the backend sends this information via the currentStepFromBackend\n//       setCurrentStep(currentStep + 1);\n//       playNextAudio();\n//       return;\n//     }\n\n//     // Step 9: Detect marker ID=1 and marker ID=3\n//     if (currentStep === 9 && ids.includes(1) && ids.includes(3)) {\n//       console.log('Step 9: Detected both Marker 1 and Marker 3');\n//       // Assume the backend sends this information via the currentStepFromBackend\n//       setCurrentStep(currentStep + 1);\n//       playNextAudio();\n//       return;\n//     }\n\n//     // Step 10: Detect marker ID=1 and marker ID=4\n//     if (currentStep === 10 && ids.includes(1) && ids.includes(4)) {\n//       console.log('Step 10: Detected both Marker 1 and Marker 4');\n//       // Assume the backend sends this information via the currentStepFromBackend\n//       setCurrentStep(currentStep + 1);\n//       playNextAudio();\n//       return;\n//     }\n\n//     const expectedMarkerId = currentStep; // Expect the next marker ID\n//     // console.log('expectedMarkerId: '+expectedMarkerId+',currentSTep: '+currentStep)\n\n//     if (ids.includes(expectedMarkerId)) {\n//       console.log(`Step ${expectedMarkerId} verified with marker ID: ${expectedMarkerId}`);\n//       setCurrentStep(currentStep + 1);\n//       playNextAudio();\n//     }\n//   };\n\n//   const playNextAudio = () => {\n//     if (currentStep + 2 >= audioRefs.current.length) return; // Prevent out-of-bounds\n\n//     const currentAudio = audioRefs.current[currentStep + 2];\n\n//     setIsAudioPlaying(true); // Set audio playing to true\n//     currentAudio.play().catch(err => console.error(\"Audio play error:\", err));\n\n//     currentAudio.onended = () => {\n//       setIsAudioPlaying(false); // Set audio playing to false after it finishes\n\n//       // Check if this is the last instructional step\n//       if (currentStep === audioRefs.current.length - 2) {\n//         setCompleted(true);\n//         audioRefs.current[audioRefs.current.length - 1].play().catch(err => console.error(\"Audio play error:\", err)); // Play completion audio\n//       }\n//     };\n//   };\n\n//   // Trigger the frame processing when the tutorial starts\n//   useEffect(() => {\n//     if (isTutorialStarted) {\n//       const interval = setInterval(processFrame, 1000 / 10);  // 10 FPS\n//       return () => clearInterval(interval);\n//     }\n//   }, [isTutorialStarted, currentStep]);\n\n//   const startTutorial = () => {\n//     setIsTutorialStarted(true);\n//     audioRefs.current[0].play().catch(err => console.error(\"Audio play error:\", err));\n//     audioRefs.current[0].onended = () => {\n//       audioRefs.current[1].play().catch(err => console.error(\"Audio play error:\", err));\n//     };\n//     setCurrentStep(0);\n//   };\n\n//   return (\n//     <div>\n//       {!isTutorialStarted && (\n//         <button onClick={startTutorial}>Start Tutorial</button>\n//       )}\n//       {isTutorialStarted && (\n//         <div>\n//           <video ref={videoRef} style={{ display: 'none' }} />\n//           <canvas ref={canvasRef} width=\"640\" height=\"480\" style={{ border: '1px solid black' }} />\n//           {completed && <p style={{ color: 'blue', marginTop: '10px' }}>Sulfate Ion Test Completed</p>}\n//         </div>\n//       )}\n//     </div>\n//   );\n// };\n\n// export default CameraFeed;\n\nimport React, { useEffect, useRef, useState } from 'react';\nimport axios from 'axios';\nimport { jsxDEV as _jsxDEV } from \"react/jsx-dev-runtime\";\nconst CameraFeed = () => {\n  _s();\n  const [detected, setDetected] = useState(false);\n  const [completed, setCompleted] = useState(false);\n  const [currentStep, setCurrentStep] = useState(0); // Track the current step\n  const [isTutorialStarted, setIsTutorialStarted] = useState(false); // Track whether tutorial has started\n  const [isAudioPlaying, setIsAudioPlaying] = useState(false); // Track if audio is playing\n  const videoRef = useRef(null);\n  const canvasRef = useRef(null);\n  const processedCanvasRef = useRef(null);\n  const overlayCanvasRef = useRef(null); // Overlay canvas for text and y-axis\n  const audioRefs = useRef([new Audio('/voice1.mp3'),\n  // Step 1: Introduction to sulfate test\n  new Audio('/voice2.mp3'),\n  // Step 2: Show distilled water\n  new Audio('/voice3.mp3'),\n  // Step 3: Show test tube 1\n  new Audio('/voice4.mp3'), new Audio('/voice5.mp3'), new Audio('/voice6.mp3'), new Audio('/voice7.mp3'), new Audio('/voice8.mp3'), new Audio('/voice9.mp3'), new Audio('/voice10.mp3'), new Audio('/voice11.mp3'), new Audio('/voice12.mp3'),\n  // Add the rest of the steps here\n  new Audio('/voice13.mp3') // Step 13: Test completion message\n  ]);\n\n  // Paragraphs for each step\n  const stepParagraphs = {\n    '-1': 'आज हम सल्फेट आयनों की पहचान के लिए एक सरल रसायन परीक्षण करेंगे। पहले, हम यह सुनिश्चित कर लेते हैं कि हमारे पास सभी आवश्यक उपकरण तैयार हैं। डिस्टिल्ड वाटर का कंटेनर दिखाएँ।',\n    0: 'टेस्ट ट्यूब 1 दिखाएँ।',\n    1: 'हाइड्रोक्लोरिक एसिड का कंटेनर दिखाएँ।',\n    2: 'बैरियम नाइट्रेट का कंटेनर दिखाएँ।',\n    3: 'सल्फ्यूरिक एसिड का कंटेनर दिखाएँ।',\n    4: 'अब हम सॉल्ट सोल्यूशन तैयार करेंगे। बैरियम नाइट्रेट सॉल्ट को टेस्ट ट्यूब 1 में डालें।',\n    5: 'डिस्टिल्ड वाटर को टेस्ट ट्यूब 1 में डालें।',\n    6: 'टेस्ट ट्यूब को जोर से हिलाएँ जब तक कि कम से कम आधा सॉल्ट घुल न जाए। कैमरे के सामने टेस्ट ट्यूब को साइडवेज़ में घुमाएँ ताकि इस स्टेप का पूरा होना संकेतित हो सके।',\n    7: 'अब हम प्रारंभिक परीक्षण करेंगे। हाइड्रोक्लोरिक एसिड से यह सुनिश्चित होता है कि सोल्यूशन में कोई कार्बोनेट आयन या अन्य बाधक पदार्थ न हो। अगर कोई प्रतिक्रिया (बुलबुले या गैस) नहीं होती, तो सल्फेट परीक्षण करें। अब टेस्ट ट्यूब 1 में सोल्यूशन में डायलूट हाइड्रोक्लोरिक एसिड की कुछ बूंदें डालें।',\n    8: 'अब हम सल्फेट परीक्षण करेंगे। टेस्ट ट्यूब 1 में सोल्यूशन में कुछ बूंदें बैरियम नाइट्रेट की डालें। अगर सफ़ेद प्रेसिपिटेट बनता है, तो इसका मतलब है कि सैंपल में सल्फेट आयन मौजूद हैं। बैरियम सल्फेट घुलनशील नहीं होता है और सल्फेट आयन की उपस्थिति में सफ़ेद प्रेसिपिटेट बनाता है।',\n    9: 'अब हम पुष्टिकारी परीक्षण करेंगे। यह सुनिश्चित करने के लिए कि बना हुआ प्रेसिपिटेट बैरियम सल्फेट ही है, न कि कोई अन्य यौगिक। अगर कोई अतिरिक्त प्रेसिपिटेट नहीं बनता है, तो सल्फेट आयन की पुष्टि हो जाती है। अब टेस्ट ट्यूब 1 में, जो बैरियम सल्फेट का सोल्यूशन है, उसमें कुछ बूंदें सल्फ्यूरिक एसिड की डालें।',\n    10: 'बधाई हो, सल्फेट आयन परीक्षण पूरा हुआ।'\n  };\n\n  // Start camera feed and process frames only when the tutorial is started\n  useEffect(() => {\n    if (isTutorialStarted) {\n      const video = videoRef.current;\n\n      // Web camera initialization logic\n      navigator.mediaDevices.getUserMedia({\n        video: true\n      }).then(stream => {\n        video.srcObject = stream;\n        video.onloadedmetadata = () => {\n          video.play();\n        };\n      }).catch(err => console.error(\"Error accessing webcam:\", err));\n\n      // Clean up: stop the camera stream when component unmounts\n      return () => {\n        if (video.srcObject) {\n          video.srcObject.getTracks().forEach(track => track.stop());\n        }\n      };\n    }\n  }, [isTutorialStarted]); // Only run this effect when the tutorial starts\n\n  // const processFrame = async () => {\n  //   if (!isTutorialStarted || completed || isAudioPlaying) return; // Stop processing if tutorial hasn't started, is completed, or audio is playing\n\n  //   const canvas = canvasRef.current;\n  //   const context = canvas.getContext('2d');\n\n  //   // Ensure video is ready and draw the video feed onto the canvas\n  //   const video = videoRef.current;\n  //   if (video.readyState === video.HAVE_ENOUGH_DATA) {\n  //     context.drawImage(video, 0, 0, canvas.width, canvas.height); // Draw live feed\n\n  //     // Send frame to backend for ArUco detection\n  //     canvas.toBlob(async (blob) => {\n  //       const formData = new FormData();\n  //       formData.append('image', blob, 'frame.jpg');\n  //       try {\n  //         const response = await axios.post('http://localhost:5000/detect', formData, { responseType: 'json' });\n  //         // console.log(\"Response from backend:\", response.data);  // Log response\n\n  //         const { detected, augmented_image, ids, currentStep } = response.data; // Destructure response\n\n  //         if (ids) {\n  //           handleMarkerDetection(ids, currentStep);\n  //         }\n\n  //         // Clear the canvas before drawing the new image\n  //         context.clearRect(0, 0, canvas.width, canvas.height);\n\n  //         // Display processed frame (augmented) coming from backend\n  //         const img = new Image();\n  //         img.src = `data:image/jpeg;base64,${augmented_image}`;\n  //         img.onload = () => {\n  //           context.drawImage(img, 0, 0, canvas.width, canvas.height);\n  //         };\n  //       } catch (err) {\n  //         console.error(\"Error processing frame:\", err);\n  //       }\n  //     }, 'image/jpeg');\n  //   }\n  // };\n\n  // const processFrame = async () => {\n  //   if (!isTutorialStarted || completed || isAudioPlaying) return; // Stop processing if tutorial hasn't started, is completed, or audio is playing\n\n  //   const canvas = canvasRef.current;\n  //   const context = canvas.getContext('2d');\n\n  //   // Ensure video is ready and draw the video feed onto the canvas\n  //   const video = videoRef.current;\n  //   if (video.readyState === video.HAVE_ENOUGH_DATA) {\n  //     context.drawImage(video, 0, 0, canvas.width, canvas.height); // Draw live feed\n\n  //     // Send frame to backend for ArUco detection\n  //     canvas.toBlob(async (blob) => {\n  //       const formData = new FormData();\n  //       formData.append('image', blob, 'frame.jpg');\n  //       try {\n  //         const response = await axios.post('http://localhost:5000/detect', formData, { responseType: 'json' });\n  //         const { detected, augmented_image, ids, currentStep } = response.data; // Destructure response\n\n  //         if (ids) {\n  //           handleMarkerDetection(ids, currentStep);\n  //         }\n\n  //         // Clear the canvas before drawing the new image\n  //         context.clearRect(0, 0, canvas.width, canvas.height);\n\n  //         // Display processed frame (augmented) coming from backend\n  //         const img = new Image();\n  //         img.src = `data:image/jpeg;base64,${augmented_image}`;\n  //         img.onload = () => {\n  //           context.drawImage(img, 0, 0, canvas.width, canvas.height);\n\n  //           // Now draw the text and axis on top of the processed frame\n  //           context.beginPath();\n  //           context.moveTo(50, 0); // Example: Drawing Y-axis\n  //           context.lineTo(50, canvas.height);\n  //           context.strokeStyle = 'red';\n  //           context.lineWidth = 2;\n  //           context.stroke();\n\n  //           // Draw text after processed image is rendered\n  //           context.font = '20px Arial';\n  //           context.fillStyle = 'blue';\n  //           context.fillText('Detected Marker', 100, 100); // Example: Text overlay\n  //         };\n  //       } catch (err) {\n  //         console.error(\"Error processing frame:\", err);\n  //       }\n  //     }, 'image/jpeg');\n  //   }\n  // };\n\n  const processFrame = async () => {\n    if (!isTutorialStarted || completed || isAudioPlaying) return;\n    const canvas = canvasRef.current;\n    const context = canvas.getContext('2d');\n\n    // Create an offscreen canvas for double buffering\n    const offscreenCanvas = document.createElement('canvas');\n    const offscreenContext = offscreenCanvas.getContext('2d');\n    offscreenCanvas.width = canvas.width;\n    offscreenCanvas.height = canvas.height;\n\n    // Ensure the video is ready and draw the video feed onto the offscreen canvas\n    const video = videoRef.current;\n    if (video.readyState === video.HAVE_ENOUGH_DATA) {\n      offscreenContext.drawImage(video, 0, 0, offscreenCanvas.width, offscreenCanvas.height);\n\n      // Send frame to backend for ArUco detection\n      offscreenCanvas.toBlob(async blob => {\n        const formData = new FormData();\n        formData.append('image', blob, 'frame.jpg');\n        try {\n          const response = await axios.post('http://localhost:5000/detect', formData, {\n            responseType: 'json'\n          });\n          const {\n            detected,\n            augmented_image,\n            ids,\n            currentStep\n          } = response.data;\n          if (ids) {\n            handleMarkerDetection(ids, currentStep);\n          }\n\n          // Load the augmented image (processed frame from backend)\n          const img = new Image();\n          img.src = `data:image/jpeg;base64,${augmented_image}`;\n          img.onload = () => {\n            // Draw the augmented image on the offscreen canvas\n            offscreenContext.drawImage(img, 0, 0, offscreenCanvas.width, offscreenCanvas.height);\n\n            // // Now draw the additional elements (text, y-axis) on the offscreen canvas\n            // offscreenContext.beginPath();\n            // offscreenContext.moveTo(50, 0); // Example: Drawing Y-axis\n            // offscreenContext.lineTo(50, offscreenCanvas.height);\n            // offscreenContext.strokeStyle = 'red';\n            // offscreenContext.lineWidth = 2;\n            // offscreenContext.stroke();\n\n            // offscreenContext.font = '20px Arial';\n            // offscreenContext.fillStyle = 'blue';\n            // offscreenContext.fillText('Detected Marker', 100, 100); // Example: Text overlay\n\n            // Now copy the entire offscreen canvas to the visible canvas in one step\n            context.clearRect(0, 0, canvas.width, canvas.height); // Clear the main canvas\n            context.drawImage(offscreenCanvas, 0, 0, canvas.width, canvas.height);\n          };\n        } catch (err) {\n          console.error(\"Error processing frame:\", err);\n        }\n      }, 'image/jpeg');\n    }\n  };\n  const drawOverlay = () => {\n    const overlayCanvas = overlayCanvasRef.current;\n    const overlayContext = overlayCanvas.getContext('2d');\n\n    // Clear any previous overlay drawings\n    overlayContext.clearRect(0, 0, overlayCanvas.width, overlayCanvas.height);\n\n    // Draw Y-axis (or any axis)\n    overlayContext.beginPath();\n    overlayContext.moveTo(50, 0);\n    overlayContext.lineTo(50, overlayCanvas.height);\n    overlayContext.strokeStyle = 'red';\n    overlayContext.lineWidth = 2;\n    overlayContext.stroke();\n\n    // Draw text\n    overlayContext.font = '20px Arial';\n    overlayContext.fillStyle = 'blue';\n    overlayContext.fillText('Detected Marker', 100, 100); // Example text\n  };\n  const handleMarkerDetection = (ids, currentStepFromBackend) => {\n    if (completed) return;\n    // Stop if completed\n\n    // For step 5, ensure both IDs 1 and 3 are present\n    if (currentStep === 5 && ids.includes(1) && ids.includes(3)) {\n      console.log('Step 5: Detected both Marker 1 and Marker 3');\n      setCurrentStep(currentStep + 1);\n      playNextAudio();\n      return;\n    }\n    if (currentStep === 6 && ids.includes(0) && ids.includes(1)) {\n      console.log('Step 6: Detected both Marker 0 and Marker 1');\n      setCurrentStep(currentStep + 1);\n      playNextAudio();\n      return;\n    }\n\n    // Step 7: Detect marker ID=1 with alternating y-axis conditions\n    if (currentStep === 7 && ids.includes(1) && !ids.includes(0)) {\n      console.log('Step 7: Marker 1 detected, alternating y-axis required');\n      // Assume the backend sends this information via the currentStepFromBackend\n      setCurrentStep(currentStep + 1);\n      playNextAudio();\n      return;\n    }\n\n    // Step 8: Detect marker ID=1 with alternating y-axis conditions\n    if (currentStep === 8 && ids.includes(1) && ids.includes(2)) {\n      console.log('Step 8: Detected both Marker 1 and Marker 2');\n      // Assume the backend sends this information via the currentStepFromBackend\n      setCurrentStep(currentStep + 1);\n      playNextAudio();\n      return;\n    }\n\n    // Step 9: Detect marker ID=1 and marker ID=3\n    if (currentStep === 9 && ids.includes(1) && ids.includes(3)) {\n      console.log('Step 9: Detected both Marker 1 and Marker 3');\n      // Assume the backend sends this information via the currentStepFromBackend\n      setCurrentStep(currentStep + 1);\n      playNextAudio();\n      return;\n    }\n\n    // Step 10: Detect marker ID=1 and marker ID=4\n    if (currentStep === 10 && ids.includes(1) && ids.includes(4)) {\n      console.log('Step 10: Detected both Marker 1 and Marker 4');\n      // Assume the backend sends this information via the currentStepFromBackend\n      setCurrentStep(currentStep + 1);\n      playNextAudio();\n      return;\n    }\n    const expectedMarkerId = currentStep; // Expect the next marker ID\n\n    if (ids.includes(expectedMarkerId)) {\n      console.log(`Step ${expectedMarkerId} verified with marker ID: ${expectedMarkerId}`);\n      setCurrentStep(currentStep + 1);\n      playNextAudio();\n    }\n  };\n  const playNextAudio = () => {\n    if (currentStep + 2 >= audioRefs.current.length) return; // Prevent out-of-bounds\n\n    const currentAudio = audioRefs.current[currentStep + 2];\n    setIsAudioPlaying(true); // Set audio playing to true\n    currentAudio.play().catch(err => console.error(\"Audio play error:\", err));\n    currentAudio.onended = () => {\n      setIsAudioPlaying(false); // Set audio playing to false after it finishes\n\n      // Check if this is the last instructional step\n      if (currentStep === audioRefs.current.length - 2) {\n        setCompleted(true);\n        audioRefs.current[audioRefs.current.length - 1].play().catch(err => console.error(\"Audio play error:\", err)); // Play completion audio\n      }\n    };\n  };\n\n  // Trigger the frame processing when the tutorial starts\n  useEffect(() => {\n    if (isTutorialStarted) {\n      const interval = setInterval(processFrame, 1000 / 10); // 10 FPS\n      return () => clearInterval(interval);\n    }\n  }, [isTutorialStarted, currentStep, isAudioPlaying]); // Added isAudioPlaying to the dependency array\n\n  const startTutorial = () => {\n    setIsTutorialStarted(true);\n    audioRefs.current[0].play().catch(err => console.error(\"Audio play error:\", err));\n    audioRefs.current[0].onended = () => {\n      audioRefs.current[1].play().catch(err => console.error(\"Audio play error:\", err));\n    };\n    setCurrentStep(0);\n  };\n  return /*#__PURE__*/_jsxDEV(\"div\", {\n    children: [!isTutorialStarted && /*#__PURE__*/_jsxDEV(\"button\", {\n      onClick: startTutorial,\n      children: \"Start Tutorial\"\n    }, void 0, false, {\n      fileName: _jsxFileName,\n      lineNumber: 579,\n      columnNumber: 9\n    }, this), isTutorialStarted && /*#__PURE__*/_jsxDEV(\"div\", {\n      children: [/*#__PURE__*/_jsxDEV(\"video\", {\n        ref: videoRef,\n        style: {\n          display: 'none'\n        }\n      }, void 0, false, {\n        fileName: _jsxFileName,\n        lineNumber: 583,\n        columnNumber: 11\n      }, this), /*#__PURE__*/_jsxDEV(\"canvas\", {\n        ref: canvasRef,\n        width: \"640\",\n        height: \"480\",\n        style: {\n          border: '1px solid black'\n        }\n      }, void 0, false, {\n        fileName: _jsxFileName,\n        lineNumber: 584,\n        columnNumber: 11\n      }, this), /*#__PURE__*/_jsxDEV(\"p\", {\n        style: {\n          marginTop: '10px'\n        },\n        children: stepParagraphs[currentStep - 1] || 'Loading...'\n      }, void 0, false, {\n        fileName: _jsxFileName,\n        lineNumber: 585,\n        columnNumber: 11\n      }, this), completed && /*#__PURE__*/_jsxDEV(\"p\", {\n        style: {\n          color: 'blue',\n          marginTop: '10px'\n        },\n        children: \"Sulfate Ion Test Completed\"\n      }, void 0, false, {\n        fileName: _jsxFileName,\n        lineNumber: 588,\n        columnNumber: 25\n      }, this)]\n    }, void 0, true, {\n      fileName: _jsxFileName,\n      lineNumber: 582,\n      columnNumber: 9\n    }, this)]\n  }, void 0, true, {\n    fileName: _jsxFileName,\n    lineNumber: 577,\n    columnNumber: 5\n  }, this);\n};\n_s(CameraFeed, \"qi1lSAylw9G5ikM/d1ajZBfJi1Q=\");\n_c = CameraFeed;\nexport default CameraFeed;\nvar _c;\n$RefreshReg$(_c, \"CameraFeed\");","map":{"version":3,"names":["React","useEffect","useRef","useState","axios","jsxDEV","_jsxDEV","CameraFeed","_s","detected","setDetected","completed","setCompleted","currentStep","setCurrentStep","isTutorialStarted","setIsTutorialStarted","isAudioPlaying","setIsAudioPlaying","videoRef","canvasRef","processedCanvasRef","overlayCanvasRef","audioRefs","Audio","stepParagraphs","video","current","navigator","mediaDevices","getUserMedia","then","stream","srcObject","onloadedmetadata","play","catch","err","console","error","getTracks","forEach","track","stop","processFrame","canvas","context","getContext","offscreenCanvas","document","createElement","offscreenContext","width","height","readyState","HAVE_ENOUGH_DATA","drawImage","toBlob","blob","formData","FormData","append","response","post","responseType","augmented_image","ids","data","handleMarkerDetection","img","Image","src","onload","clearRect","drawOverlay","overlayCanvas","overlayContext","beginPath","moveTo","lineTo","strokeStyle","lineWidth","stroke","font","fillStyle","fillText","currentStepFromBackend","includes","log","playNextAudio","expectedMarkerId","length","currentAudio","onended","interval","setInterval","clearInterval","startTutorial","children","onClick","fileName","_jsxFileName","lineNumber","columnNumber","ref","style","display","border","marginTop","color","_c","$RefreshReg$"],"sources":["/home/user/Documents/Aruco_POC/aruco-detector/src/CameraFeed.js"],"sourcesContent":["\n// import React, { useEffect, useRef, useState } from 'react';\n// import axios from 'axios';\n\n// const CameraFeed = () => {\n//   const [detected, setDetected] = useState(false);\n//   const [completed, setCompleted] = useState(false);\n//   const [currentStep, setCurrentStep] = useState(0);  // Track the current step\n//   const [isTutorialStarted, setIsTutorialStarted] = useState(false); // Track whether tutorial has started\n//   const [isAudioPlaying, setIsAudioPlaying] = useState(false);\n//   const videoRef = useRef(null);\n//   const canvasRef = useRef(null);\n//   const audioRefs = useRef([\n//     new Audio('/voice1.mp3'), // Step 1: Introduction to sulfate test\n//     new Audio('/voice2.mp3'), // Step 2: Show distilled water\n//     new Audio('/voice3.mp3'), // Step 3: Show test tube 1\n//     new Audio('/voice4.mp3'),\n//     new Audio('/voice5.mp3'),\n//     new Audio('/voice6.mp3'),\n//     new Audio('/voice7.mp3'),\n//     new Audio('/voice8.mp3'),\n//     new Audio('/voice9.mp3'),\n//     new Audio('/voice10.mp3'),\n//     new Audio('/voice11.mp3'),\n//     new Audio('/voice12.mp3'),\n//     new Audio('/voice13.mp3'), // Step 13: Test completion message\n//   ]);\n\n//   // Start camera feed and process frames only when the tutorial is started\n//   useEffect(() => {\n//     if (isTutorialStarted) {\n//       const video = videoRef.current;\n\n//       // Web camera initialization logic\n//       navigator.mediaDevices.getUserMedia({ video: true })\n//         .then((stream) => {\n//           video.srcObject = stream;\n//           video.onloadedmetadata = () => {\n//             video.play();\n//           };\n//         })\n//         .catch(err => console.error(\"Error accessing webcam:\", err));\n\n//       // Clean up: stop the camera stream when component unmounts\n//       return () => {\n//         if (video.srcObject) {\n//           video.srcObject.getTracks().forEach(track => track.stop());\n//         }\n//       };\n//     }\n//   }, [isTutorialStarted]);  // Only run this effect when the tutorial starts\n\n//   const processFrame = async () => {\n//     if (!isTutorialStarted || completed) return;  // Stop processing if the tutorial hasn't started or test is complete\n  \n//     const canvas = canvasRef.current;\n//     const context = canvas.getContext('2d');\n    \n//     // Ensure video is ready and draw the video feed onto the canvas\n//     const video = videoRef.current;\n//     if (video.readyState === video.HAVE_ENOUGH_DATA) {\n//       context.drawImage(video, 0, 0, canvas.width, canvas.height);\n  \n//       // Send frame to backend for ArUco detection\n//       canvas.toBlob(async (blob) => {\n//         const formData = new FormData();\n//         formData.append('image', blob, 'frame.jpg');\n//         try {\n//           const response = await axios.post('http://localhost:5000/detect', formData, { responseType: 'json' });\n//           // console.log(\"Response from backend:\", response.data);  // Log response\n  \n//           const { detected, augmented_image, ids, currentStep } = response.data;  // Destructure response\n          \n//           if (ids) {\n//               handleMarkerDetection(ids, currentStep);\n//             }\n\n//           // Clear the canvas before drawing the new image\n//           context.clearRect(0, 0, canvas.width, canvas.height);\n  \n//           // Display processed frame (augmented) coming from backend\n//           const img = new Image();\n//           img.src = `data:image/jpeg;base64,${augmented_image}`;\n//           img.onload = () => {\n//             context.drawImage(img, 0, 0, canvas.width, canvas.height);\n//           };\n//         } catch (err) {\n//           console.error(\"Error processing frame:\", err);\n//         }\n//       }, 'image/jpeg');\n//     }\n//   };\n\n\n\n//   const handleMarkerDetection = (ids,currentStepFromBackend) => {\n//     if (completed) return;\n//     // console.log('inside handleMarkerDetection');\n//      // Stop if completed\n\n\n//         // For step 5, ensure both IDs 1 and 3 are present\n//     if (currentStep === 5 && ids.includes(1) && ids.includes(3)) {\n//       console.log('Step 5: Detected both Marker 1 and Marker 3');\n//       setCurrentStep(currentStep + 1);\n//       playNextAudio();\n//       return;\n//     }\n\n//     if (currentStep === 6 && ids.includes(0) && ids.includes(1)) {\n//       console.log('Step 6: Detected both Marker 0 and Marker 1');\n//       setCurrentStep(currentStep + 1);\n//       playNextAudio();\n//       return;\n//     }\n\n//     // Step 7: Detect marker ID=1 with alternating y-axis conditions\n//     if (currentStep === 7 && ids.includes(1) && !ids.includes(0)) {\n//       console.log('Step 7: Marker 1 detected, alternating y-axis required');\n//       // Assume the backend sends this information via the currentStepFromBackend\n//       setCurrentStep(currentStep + 1);\n//       playNextAudio();\n//       return;\n//     }\n\n//     // Step 8: Detect marker ID=1 with alternating y-axis conditions\n//     if (currentStep === 8 && ids.includes(1) && ids.includes(2)) {\n//       console.log('Step 8: Detected both Marker 1 and Marker 2');\n//       // Assume the backend sends this information via the currentStepFromBackend\n//       setCurrentStep(currentStep + 1);\n//       playNextAudio();\n//       return;\n//     }\n\n//     // Step 9: Detect marker ID=1 and marker ID=3\n//     if (currentStep === 9 && ids.includes(1) && ids.includes(3)) {\n//       console.log('Step 9: Detected both Marker 1 and Marker 3');\n//       // Assume the backend sends this information via the currentStepFromBackend\n//       setCurrentStep(currentStep + 1);\n//       playNextAudio();\n//       return;\n//     }\n\n//     // Step 10: Detect marker ID=1 and marker ID=4\n//     if (currentStep === 10 && ids.includes(1) && ids.includes(4)) {\n//       console.log('Step 10: Detected both Marker 1 and Marker 4');\n//       // Assume the backend sends this information via the currentStepFromBackend\n//       setCurrentStep(currentStep + 1);\n//       playNextAudio();\n//       return;\n//     }\n\n//     const expectedMarkerId = currentStep; // Expect the next marker ID\n//     // console.log('expectedMarkerId: '+expectedMarkerId+',currentSTep: '+currentStep)\n\n//     if (ids.includes(expectedMarkerId)) {\n//       console.log(`Step ${expectedMarkerId} verified with marker ID: ${expectedMarkerId}`);\n//       setCurrentStep(currentStep + 1);\n//       playNextAudio();\n//     }\n//   };\n\n//   const playNextAudio = () => {\n//     if (currentStep + 2 >= audioRefs.current.length) return; // Prevent out-of-bounds\n    \n    \n//     const currentAudio = audioRefs.current[currentStep + 2];\n\n    \n//     setIsAudioPlaying(true); // Set audio playing to true\n//     currentAudio.play().catch(err => console.error(\"Audio play error:\", err));\n    \n//     currentAudio.onended = () => {\n//       setIsAudioPlaying(false); // Set audio playing to false after it finishes\n  \n//       // Check if this is the last instructional step\n//       if (currentStep === audioRefs.current.length - 2) {\n//         setCompleted(true);\n//         audioRefs.current[audioRefs.current.length - 1].play().catch(err => console.error(\"Audio play error:\", err)); // Play completion audio\n//       }\n//     };\n//   };\n  \n  \n\n  \n//   // Trigger the frame processing when the tutorial starts\n//   useEffect(() => {\n//     if (isTutorialStarted) {\n//       const interval = setInterval(processFrame, 1000 / 10);  // 10 FPS\n//       return () => clearInterval(interval);\n//     }\n//   }, [isTutorialStarted, currentStep]);\n\n\n//   const startTutorial = () => {\n//     setIsTutorialStarted(true);\n//     audioRefs.current[0].play().catch(err => console.error(\"Audio play error:\", err));\n//     audioRefs.current[0].onended = () => {\n//       audioRefs.current[1].play().catch(err => console.error(\"Audio play error:\", err));\n//     };\n//     setCurrentStep(0);\n//   };\n  \n\n//   return (\n//     <div>\n//       {!isTutorialStarted && (\n//         <button onClick={startTutorial}>Start Tutorial</button>\n//       )}\n//       {isTutorialStarted && (\n//         <div>\n//           <video ref={videoRef} style={{ display: 'none' }} />\n//           <canvas ref={canvasRef} width=\"640\" height=\"480\" style={{ border: '1px solid black' }} />\n//           {completed && <p style={{ color: 'blue', marginTop: '10px' }}>Sulfate Ion Test Completed</p>}\n//         </div>\n//       )}\n//     </div>\n//   );\n// };\n\n// export default CameraFeed;\n\n\n\n\n\n\n\nimport React, { useEffect, useRef, useState } from 'react';\nimport axios from 'axios';\n\nconst CameraFeed = () => {\n  const [detected, setDetected] = useState(false);\n  const [completed, setCompleted] = useState(false);\n  const [currentStep, setCurrentStep] = useState(0); // Track the current step\n  const [isTutorialStarted, setIsTutorialStarted] = useState(false); // Track whether tutorial has started\n  const [isAudioPlaying, setIsAudioPlaying] = useState(false); // Track if audio is playing\n  const videoRef = useRef(null);\n  const canvasRef = useRef(null);\n  const processedCanvasRef = useRef(null);\n  const overlayCanvasRef = useRef(null); // Overlay canvas for text and y-axis\n  const audioRefs = useRef([\n    new Audio('/voice1.mp3'), // Step 1: Introduction to sulfate test\n    new Audio('/voice2.mp3'), // Step 2: Show distilled water\n    new Audio('/voice3.mp3'), // Step 3: Show test tube 1\n    new Audio('/voice4.mp3'),\n    new Audio('/voice5.mp3'),\n    new Audio('/voice6.mp3'),\n    new Audio('/voice7.mp3'),\n    new Audio('/voice8.mp3'),\n    new Audio('/voice9.mp3'),\n    new Audio('/voice10.mp3'),\n    new Audio('/voice11.mp3'),\n    new Audio('/voice12.mp3'),\n    // Add the rest of the steps here\n    new Audio('/voice13.mp3'), // Step 13: Test completion message\n  ]);\n\n    // Paragraphs for each step\n    const stepParagraphs = {\n      '-1': 'आज हम सल्फेट आयनों की पहचान के लिए एक सरल रसायन परीक्षण करेंगे। पहले, हम यह सुनिश्चित कर लेते हैं कि हमारे पास सभी आवश्यक उपकरण तैयार हैं। डिस्टिल्ड वाटर का कंटेनर दिखाएँ।',\n      0: 'टेस्ट ट्यूब 1 दिखाएँ।',\n      1: 'हाइड्रोक्लोरिक एसिड का कंटेनर दिखाएँ।',\n      2: 'बैरियम नाइट्रेट का कंटेनर दिखाएँ।',\n      3: 'सल्फ्यूरिक एसिड का कंटेनर दिखाएँ।',\n      4: 'अब हम सॉल्ट सोल्यूशन तैयार करेंगे। बैरियम नाइट्रेट सॉल्ट को टेस्ट ट्यूब 1 में डालें।',\n      5: 'डिस्टिल्ड वाटर को टेस्ट ट्यूब 1 में डालें।',\n      6: 'टेस्ट ट्यूब को जोर से हिलाएँ जब तक कि कम से कम आधा सॉल्ट घुल न जाए। कैमरे के सामने टेस्ट ट्यूब को साइडवेज़ में घुमाएँ ताकि इस स्टेप का पूरा होना संकेतित हो सके।',\n      7: 'अब हम प्रारंभिक परीक्षण करेंगे। हाइड्रोक्लोरिक एसिड से यह सुनिश्चित होता है कि सोल्यूशन में कोई कार्बोनेट आयन या अन्य बाधक पदार्थ न हो। अगर कोई प्रतिक्रिया (बुलबुले या गैस) नहीं होती, तो सल्फेट परीक्षण करें। अब टेस्ट ट्यूब 1 में सोल्यूशन में डायलूट हाइड्रोक्लोरिक एसिड की कुछ बूंदें डालें।',\n      8: 'अब हम सल्फेट परीक्षण करेंगे। टेस्ट ट्यूब 1 में सोल्यूशन में कुछ बूंदें बैरियम नाइट्रेट की डालें। अगर सफ़ेद प्रेसिपिटेट बनता है, तो इसका मतलब है कि सैंपल में सल्फेट आयन मौजूद हैं। बैरियम सल्फेट घुलनशील नहीं होता है और सल्फेट आयन की उपस्थिति में सफ़ेद प्रेसिपिटेट बनाता है।',\n      9: 'अब हम पुष्टिकारी परीक्षण करेंगे। यह सुनिश्चित करने के लिए कि बना हुआ प्रेसिपिटेट बैरियम सल्फेट ही है, न कि कोई अन्य यौगिक। अगर कोई अतिरिक्त प्रेसिपिटेट नहीं बनता है, तो सल्फेट आयन की पुष्टि हो जाती है। अब टेस्ट ट्यूब 1 में, जो बैरियम सल्फेट का सोल्यूशन है, उसमें कुछ बूंदें सल्फ्यूरिक एसिड की डालें।',\n      10: 'बधाई हो, सल्फेट आयन परीक्षण पूरा हुआ।'\n    };\n\n  // Start camera feed and process frames only when the tutorial is started\n  useEffect(() => {\n    if (isTutorialStarted) {\n      const video = videoRef.current;\n\n      // Web camera initialization logic\n      navigator.mediaDevices.getUserMedia({ video: true })\n        .then((stream) => {\n          video.srcObject = stream;\n          video.onloadedmetadata = () => {\n            video.play();\n          };\n        })\n        .catch(err => console.error(\"Error accessing webcam:\", err));\n\n      // Clean up: stop the camera stream when component unmounts\n      return () => {\n        if (video.srcObject) {\n          video.srcObject.getTracks().forEach(track => track.stop());\n        }\n      };\n    }\n  }, [isTutorialStarted]); // Only run this effect when the tutorial starts\n\n  // const processFrame = async () => {\n  //   if (!isTutorialStarted || completed || isAudioPlaying) return; // Stop processing if tutorial hasn't started, is completed, or audio is playing\n\n  //   const canvas = canvasRef.current;\n  //   const context = canvas.getContext('2d');\n\n  //   // Ensure video is ready and draw the video feed onto the canvas\n  //   const video = videoRef.current;\n  //   if (video.readyState === video.HAVE_ENOUGH_DATA) {\n  //     context.drawImage(video, 0, 0, canvas.width, canvas.height); // Draw live feed\n\n  //     // Send frame to backend for ArUco detection\n  //     canvas.toBlob(async (blob) => {\n  //       const formData = new FormData();\n  //       formData.append('image', blob, 'frame.jpg');\n  //       try {\n  //         const response = await axios.post('http://localhost:5000/detect', formData, { responseType: 'json' });\n  //         // console.log(\"Response from backend:\", response.data);  // Log response\n\n  //         const { detected, augmented_image, ids, currentStep } = response.data; // Destructure response\n\n  //         if (ids) {\n  //           handleMarkerDetection(ids, currentStep);\n  //         }\n\n  //         // Clear the canvas before drawing the new image\n  //         context.clearRect(0, 0, canvas.width, canvas.height);\n\n  //         // Display processed frame (augmented) coming from backend\n  //         const img = new Image();\n  //         img.src = `data:image/jpeg;base64,${augmented_image}`;\n  //         img.onload = () => {\n  //           context.drawImage(img, 0, 0, canvas.width, canvas.height);\n  //         };\n  //       } catch (err) {\n  //         console.error(\"Error processing frame:\", err);\n  //       }\n  //     }, 'image/jpeg');\n  //   }\n  // };\n\n\n  // const processFrame = async () => {\n  //   if (!isTutorialStarted || completed || isAudioPlaying) return; // Stop processing if tutorial hasn't started, is completed, or audio is playing\n  \n  //   const canvas = canvasRef.current;\n  //   const context = canvas.getContext('2d');\n  \n  //   // Ensure video is ready and draw the video feed onto the canvas\n  //   const video = videoRef.current;\n  //   if (video.readyState === video.HAVE_ENOUGH_DATA) {\n  //     context.drawImage(video, 0, 0, canvas.width, canvas.height); // Draw live feed\n  \n  //     // Send frame to backend for ArUco detection\n  //     canvas.toBlob(async (blob) => {\n  //       const formData = new FormData();\n  //       formData.append('image', blob, 'frame.jpg');\n  //       try {\n  //         const response = await axios.post('http://localhost:5000/detect', formData, { responseType: 'json' });\n  //         const { detected, augmented_image, ids, currentStep } = response.data; // Destructure response\n  \n  //         if (ids) {\n  //           handleMarkerDetection(ids, currentStep);\n  //         }\n  \n  //         // Clear the canvas before drawing the new image\n  //         context.clearRect(0, 0, canvas.width, canvas.height);\n  \n  //         // Display processed frame (augmented) coming from backend\n  //         const img = new Image();\n  //         img.src = `data:image/jpeg;base64,${augmented_image}`;\n  //         img.onload = () => {\n  //           context.drawImage(img, 0, 0, canvas.width, canvas.height);\n  \n  //           // Now draw the text and axis on top of the processed frame\n  //           context.beginPath();\n  //           context.moveTo(50, 0); // Example: Drawing Y-axis\n  //           context.lineTo(50, canvas.height);\n  //           context.strokeStyle = 'red';\n  //           context.lineWidth = 2;\n  //           context.stroke();\n  \n  //           // Draw text after processed image is rendered\n  //           context.font = '20px Arial';\n  //           context.fillStyle = 'blue';\n  //           context.fillText('Detected Marker', 100, 100); // Example: Text overlay\n  //         };\n  //       } catch (err) {\n  //         console.error(\"Error processing frame:\", err);\n  //       }\n  //     }, 'image/jpeg');\n  //   }\n  // };\n\n  const processFrame = async () => {\n    if (!isTutorialStarted || completed || isAudioPlaying) return;\n  \n    const canvas = canvasRef.current;\n    const context = canvas.getContext('2d');\n  \n    // Create an offscreen canvas for double buffering\n    const offscreenCanvas = document.createElement('canvas');\n    const offscreenContext = offscreenCanvas.getContext('2d');\n    offscreenCanvas.width = canvas.width;\n    offscreenCanvas.height = canvas.height;\n  \n    // Ensure the video is ready and draw the video feed onto the offscreen canvas\n    const video = videoRef.current;\n    if (video.readyState === video.HAVE_ENOUGH_DATA) {\n      offscreenContext.drawImage(video, 0, 0, offscreenCanvas.width, offscreenCanvas.height);\n  \n      // Send frame to backend for ArUco detection\n      offscreenCanvas.toBlob(async (blob) => {\n        const formData = new FormData();\n        formData.append('image', blob, 'frame.jpg');\n        try {\n          const response = await axios.post('http://localhost:5000/detect', formData, { responseType: 'json' });\n          const { detected, augmented_image, ids, currentStep } = response.data;\n  \n          if (ids) {\n            handleMarkerDetection(ids, currentStep);\n          }\n  \n          // Load the augmented image (processed frame from backend)\n          const img = new Image();\n          img.src = `data:image/jpeg;base64,${augmented_image}`;\n          img.onload = () => {\n            // Draw the augmented image on the offscreen canvas\n            offscreenContext.drawImage(img, 0, 0, offscreenCanvas.width, offscreenCanvas.height);\n  \n            // // Now draw the additional elements (text, y-axis) on the offscreen canvas\n            // offscreenContext.beginPath();\n            // offscreenContext.moveTo(50, 0); // Example: Drawing Y-axis\n            // offscreenContext.lineTo(50, offscreenCanvas.height);\n            // offscreenContext.strokeStyle = 'red';\n            // offscreenContext.lineWidth = 2;\n            // offscreenContext.stroke();\n  \n            // offscreenContext.font = '20px Arial';\n            // offscreenContext.fillStyle = 'blue';\n            // offscreenContext.fillText('Detected Marker', 100, 100); // Example: Text overlay\n  \n            // Now copy the entire offscreen canvas to the visible canvas in one step\n            context.clearRect(0, 0, canvas.width, canvas.height); // Clear the main canvas\n            context.drawImage(offscreenCanvas, 0, 0, canvas.width, canvas.height);\n          };\n        } catch (err) {\n          console.error(\"Error processing frame:\", err);\n        }\n      }, 'image/jpeg');\n    }\n  };\n  \n  \n\n    const drawOverlay = () => {\n      const overlayCanvas = overlayCanvasRef.current;\n      const overlayContext = overlayCanvas.getContext('2d');\n      \n      // Clear any previous overlay drawings\n      overlayContext.clearRect(0, 0, overlayCanvas.width, overlayCanvas.height);\n      \n      // Draw Y-axis (or any axis)\n      overlayContext.beginPath();\n      overlayContext.moveTo(50, 0);\n      overlayContext.lineTo(50, overlayCanvas.height);\n      overlayContext.strokeStyle = 'red';\n      overlayContext.lineWidth = 2;\n      overlayContext.stroke();\n  \n      // Draw text\n      overlayContext.font = '20px Arial';\n      overlayContext.fillStyle = 'blue';\n      overlayContext.fillText('Detected Marker', 100, 100); // Example text\n    };\n\n  const handleMarkerDetection = (ids, currentStepFromBackend) => {\n    if (completed) return;\n    // Stop if completed\n\n    // For step 5, ensure both IDs 1 and 3 are present\n    if (currentStep === 5 && ids.includes(1) && ids.includes(3)) {\n      console.log('Step 5: Detected both Marker 1 and Marker 3');\n      setCurrentStep(currentStep + 1);\n      playNextAudio();\n      return;\n    }\n\n    if (currentStep === 6 && ids.includes(0) && ids.includes(1)) {\n      console.log('Step 6: Detected both Marker 0 and Marker 1');\n      setCurrentStep(currentStep + 1);\n      playNextAudio();\n      return;\n    }\n\n    // Step 7: Detect marker ID=1 with alternating y-axis conditions\n    if (currentStep === 7 && ids.includes(1) && !ids.includes(0)) {\n      console.log('Step 7: Marker 1 detected, alternating y-axis required');\n      // Assume the backend sends this information via the currentStepFromBackend\n      setCurrentStep(currentStep + 1);\n      playNextAudio();\n      return;\n    }\n\n    // Step 8: Detect marker ID=1 with alternating y-axis conditions\n    if (currentStep === 8 && ids.includes(1) && ids.includes(2)) {\n      console.log('Step 8: Detected both Marker 1 and Marker 2');\n      // Assume the backend sends this information via the currentStepFromBackend\n      setCurrentStep(currentStep + 1);\n      playNextAudio();\n      return;\n    }\n\n    // Step 9: Detect marker ID=1 and marker ID=3\n    if (currentStep === 9 && ids.includes(1) && ids.includes(3)) {\n      console.log('Step 9: Detected both Marker 1 and Marker 3');\n      // Assume the backend sends this information via the currentStepFromBackend\n      setCurrentStep(currentStep + 1);\n      playNextAudio();\n      return;\n    }\n\n    // Step 10: Detect marker ID=1 and marker ID=4\n    if (currentStep === 10 && ids.includes(1) && ids.includes(4)) {\n      console.log('Step 10: Detected both Marker 1 and Marker 4');\n      // Assume the backend sends this information via the currentStepFromBackend\n      setCurrentStep(currentStep + 1);\n      playNextAudio();\n      return;\n    }\n\n    const expectedMarkerId = currentStep; // Expect the next marker ID\n\n    if (ids.includes(expectedMarkerId)) {\n      console.log(`Step ${expectedMarkerId} verified with marker ID: ${expectedMarkerId}`);\n      setCurrentStep(currentStep + 1);\n      playNextAudio();\n    }\n  };\n\n  const playNextAudio = () => {\n    if (currentStep + 2 >= audioRefs.current.length) return; // Prevent out-of-bounds\n\n    const currentAudio = audioRefs.current[currentStep + 2];\n\n    setIsAudioPlaying(true); // Set audio playing to true\n    currentAudio.play().catch(err => console.error(\"Audio play error:\", err));\n\n    currentAudio.onended = () => {\n      setIsAudioPlaying(false); // Set audio playing to false after it finishes\n\n      // Check if this is the last instructional step\n      if (currentStep === audioRefs.current.length - 2) {\n        setCompleted(true);\n        audioRefs.current[audioRefs.current.length - 1].play().catch(err => console.error(\"Audio play error:\", err)); // Play completion audio\n      }\n    };\n  };\n\n  // Trigger the frame processing when the tutorial starts\n  useEffect(() => {\n    if (isTutorialStarted) {\n      const interval = setInterval(processFrame, 1000 / 10); // 10 FPS\n      return () => clearInterval(interval);\n    }\n  }, [isTutorialStarted, currentStep, isAudioPlaying]); // Added isAudioPlaying to the dependency array\n\n  const startTutorial = () => {\n    setIsTutorialStarted(true);\n    audioRefs.current[0].play().catch(err => console.error(\"Audio play error:\", err));\n    audioRefs.current[0].onended = () => {\n      audioRefs.current[1].play().catch(err => console.error(\"Audio play error:\", err));\n    };\n    setCurrentStep(0);\n  };\n\n  return (\n    <div>\n      {!isTutorialStarted && (\n        <button onClick={startTutorial}>Start Tutorial</button>\n      )}\n      {isTutorialStarted && (\n        <div>\n          <video ref={videoRef} style={{ display: 'none' }} />\n          <canvas ref={canvasRef} width=\"640\" height=\"480\" style={{ border: '1px solid black' }} />\n          <p style={{ marginTop: '10px' }}>\n            {stepParagraphs[currentStep - 1] || 'Loading...'}\n          </p>\n          {completed && <p style={{ color: 'blue', marginTop: '10px' }}>Sulfate Ion Test Completed</p>}\n        </div>\n      )}\n    </div>\n  );\n};\n\nexport default CameraFeed;\n"],"mappings":";;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;;AAEA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAIA;AACA;AACA;AACA;;AAGA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAGA;;AAGA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;;AAKA;AACA;AACA;AACA;AACA;AACA;AACA;;AAGA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAGA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;;AAQA,OAAOA,KAAK,IAAIC,SAAS,EAAEC,MAAM,EAAEC,QAAQ,QAAQ,OAAO;AAC1D,OAAOC,KAAK,MAAM,OAAO;AAAC,SAAAC,MAAA,IAAAC,OAAA;AAE1B,MAAMC,UAAU,GAAGA,CAAA,KAAM;EAAAC,EAAA;EACvB,MAAM,CAACC,QAAQ,EAAEC,WAAW,CAAC,GAAGP,QAAQ,CAAC,KAAK,CAAC;EAC/C,MAAM,CAACQ,SAAS,EAAEC,YAAY,CAAC,GAAGT,QAAQ,CAAC,KAAK,CAAC;EACjD,MAAM,CAACU,WAAW,EAAEC,cAAc,CAAC,GAAGX,QAAQ,CAAC,CAAC,CAAC,CAAC,CAAC;EACnD,MAAM,CAACY,iBAAiB,EAAEC,oBAAoB,CAAC,GAAGb,QAAQ,CAAC,KAAK,CAAC,CAAC,CAAC;EACnE,MAAM,CAACc,cAAc,EAAEC,iBAAiB,CAAC,GAAGf,QAAQ,CAAC,KAAK,CAAC,CAAC,CAAC;EAC7D,MAAMgB,QAAQ,GAAGjB,MAAM,CAAC,IAAI,CAAC;EAC7B,MAAMkB,SAAS,GAAGlB,MAAM,CAAC,IAAI,CAAC;EAC9B,MAAMmB,kBAAkB,GAAGnB,MAAM,CAAC,IAAI,CAAC;EACvC,MAAMoB,gBAAgB,GAAGpB,MAAM,CAAC,IAAI,CAAC,CAAC,CAAC;EACvC,MAAMqB,SAAS,GAAGrB,MAAM,CAAC,CACvB,IAAIsB,KAAK,CAAC,aAAa,CAAC;EAAE;EAC1B,IAAIA,KAAK,CAAC,aAAa,CAAC;EAAE;EAC1B,IAAIA,KAAK,CAAC,aAAa,CAAC;EAAE;EAC1B,IAAIA,KAAK,CAAC,aAAa,CAAC,EACxB,IAAIA,KAAK,CAAC,aAAa,CAAC,EACxB,IAAIA,KAAK,CAAC,aAAa,CAAC,EACxB,IAAIA,KAAK,CAAC,aAAa,CAAC,EACxB,IAAIA,KAAK,CAAC,aAAa,CAAC,EACxB,IAAIA,KAAK,CAAC,aAAa,CAAC,EACxB,IAAIA,KAAK,CAAC,cAAc,CAAC,EACzB,IAAIA,KAAK,CAAC,cAAc,CAAC,EACzB,IAAIA,KAAK,CAAC,cAAc,CAAC;EACzB;EACA,IAAIA,KAAK,CAAC,cAAc,CAAC,CAAE;EAAA,CAC5B,CAAC;;EAEA;EACA,MAAMC,cAAc,GAAG;IACrB,IAAI,EAAE,6KAA6K;IACnL,CAAC,EAAE,uBAAuB;IAC1B,CAAC,EAAE,uCAAuC;IAC1C,CAAC,EAAE,mCAAmC;IACtC,CAAC,EAAE,mCAAmC;IACtC,CAAC,EAAE,sFAAsF;IACzF,CAAC,EAAE,4CAA4C;IAC/C,CAAC,EAAE,kKAAkK;IACrK,CAAC,EAAE,mSAAmS;IACtS,CAAC,EAAE,iRAAiR;IACpR,CAAC,EAAE,6SAA6S;IAChT,EAAE,EAAE;EACN,CAAC;;EAEH;EACAxB,SAAS,CAAC,MAAM;IACd,IAAIc,iBAAiB,EAAE;MACrB,MAAMW,KAAK,GAAGP,QAAQ,CAACQ,OAAO;;MAE9B;MACAC,SAAS,CAACC,YAAY,CAACC,YAAY,CAAC;QAAEJ,KAAK,EAAE;MAAK,CAAC,CAAC,CACjDK,IAAI,CAAEC,MAAM,IAAK;QAChBN,KAAK,CAACO,SAAS,GAAGD,MAAM;QACxBN,KAAK,CAACQ,gBAAgB,GAAG,MAAM;UAC7BR,KAAK,CAACS,IAAI,CAAC,CAAC;QACd,CAAC;MACH,CAAC,CAAC,CACDC,KAAK,CAACC,GAAG,IAAIC,OAAO,CAACC,KAAK,CAAC,yBAAyB,EAAEF,GAAG,CAAC,CAAC;;MAE9D;MACA,OAAO,MAAM;QACX,IAAIX,KAAK,CAACO,SAAS,EAAE;UACnBP,KAAK,CAACO,SAAS,CAACO,SAAS,CAAC,CAAC,CAACC,OAAO,CAACC,KAAK,IAAIA,KAAK,CAACC,IAAI,CAAC,CAAC,CAAC;QAC5D;MACF,CAAC;IACH;EACF,CAAC,EAAE,CAAC5B,iBAAiB,CAAC,CAAC,CAAC,CAAC;;EAEzB;EACA;;EAEA;EACA;;EAEA;EACA;EACA;EACA;;EAEA;EACA;EACA;EACA;EACA;EACA;EACA;;EAEA;;EAEA;EACA;EACA;;EAEA;EACA;;EAEA;EACA;EACA;EACA;EACA;EACA;EACA;EACA;EACA;EACA;EACA;EACA;;EAGA;EACA;;EAEA;EACA;;EAEA;EACA;EACA;EACA;;EAEA;EACA;EACA;EACA;EACA;EACA;EACA;;EAEA;EACA;EACA;;EAEA;EACA;;EAEA;EACA;EACA;EACA;EACA;;EAEA;EACA;EACA;EACA;EACA;EACA;EACA;;EAEA;EACA;EACA;EACA;EACA;EACA;EACA;EACA;EACA;EACA;EACA;;EAEA,MAAM6B,YAAY,GAAG,MAAAA,CAAA,KAAY;IAC/B,IAAI,CAAC7B,iBAAiB,IAAIJ,SAAS,IAAIM,cAAc,EAAE;IAEvD,MAAM4B,MAAM,GAAGzB,SAAS,CAACO,OAAO;IAChC,MAAMmB,OAAO,GAAGD,MAAM,CAACE,UAAU,CAAC,IAAI,CAAC;;IAEvC;IACA,MAAMC,eAAe,GAAGC,QAAQ,CAACC,aAAa,CAAC,QAAQ,CAAC;IACxD,MAAMC,gBAAgB,GAAGH,eAAe,CAACD,UAAU,CAAC,IAAI,CAAC;IACzDC,eAAe,CAACI,KAAK,GAAGP,MAAM,CAACO,KAAK;IACpCJ,eAAe,CAACK,MAAM,GAAGR,MAAM,CAACQ,MAAM;;IAEtC;IACA,MAAM3B,KAAK,GAAGP,QAAQ,CAACQ,OAAO;IAC9B,IAAID,KAAK,CAAC4B,UAAU,KAAK5B,KAAK,CAAC6B,gBAAgB,EAAE;MAC/CJ,gBAAgB,CAACK,SAAS,CAAC9B,KAAK,EAAE,CAAC,EAAE,CAAC,EAAEsB,eAAe,CAACI,KAAK,EAAEJ,eAAe,CAACK,MAAM,CAAC;;MAEtF;MACAL,eAAe,CAACS,MAAM,CAAC,MAAOC,IAAI,IAAK;QACrC,MAAMC,QAAQ,GAAG,IAAIC,QAAQ,CAAC,CAAC;QAC/BD,QAAQ,CAACE,MAAM,CAAC,OAAO,EAAEH,IAAI,EAAE,WAAW,CAAC;QAC3C,IAAI;UACF,MAAMI,QAAQ,GAAG,MAAM1D,KAAK,CAAC2D,IAAI,CAAC,8BAA8B,EAAEJ,QAAQ,EAAE;YAAEK,YAAY,EAAE;UAAO,CAAC,CAAC;UACrG,MAAM;YAAEvD,QAAQ;YAAEwD,eAAe;YAAEC,GAAG;YAAErD;UAAY,CAAC,GAAGiD,QAAQ,CAACK,IAAI;UAErE,IAAID,GAAG,EAAE;YACPE,qBAAqB,CAACF,GAAG,EAAErD,WAAW,CAAC;UACzC;;UAEA;UACA,MAAMwD,GAAG,GAAG,IAAIC,KAAK,CAAC,CAAC;UACvBD,GAAG,CAACE,GAAG,GAAG,0BAA0BN,eAAe,EAAE;UACrDI,GAAG,CAACG,MAAM,GAAG,MAAM;YACjB;YACArB,gBAAgB,CAACK,SAAS,CAACa,GAAG,EAAE,CAAC,EAAE,CAAC,EAAErB,eAAe,CAACI,KAAK,EAAEJ,eAAe,CAACK,MAAM,CAAC;;YAEpF;YACA;YACA;YACA;YACA;YACA;YACA;;YAEA;YACA;YACA;;YAEA;YACAP,OAAO,CAAC2B,SAAS,CAAC,CAAC,EAAE,CAAC,EAAE5B,MAAM,CAACO,KAAK,EAAEP,MAAM,CAACQ,MAAM,CAAC,CAAC,CAAC;YACtDP,OAAO,CAACU,SAAS,CAACR,eAAe,EAAE,CAAC,EAAE,CAAC,EAAEH,MAAM,CAACO,KAAK,EAAEP,MAAM,CAACQ,MAAM,CAAC;UACvE,CAAC;QACH,CAAC,CAAC,OAAOhB,GAAG,EAAE;UACZC,OAAO,CAACC,KAAK,CAAC,yBAAyB,EAAEF,GAAG,CAAC;QAC/C;MACF,CAAC,EAAE,YAAY,CAAC;IAClB;EACF,CAAC;EAIC,MAAMqC,WAAW,GAAGA,CAAA,KAAM;IACxB,MAAMC,aAAa,GAAGrD,gBAAgB,CAACK,OAAO;IAC9C,MAAMiD,cAAc,GAAGD,aAAa,CAAC5B,UAAU,CAAC,IAAI,CAAC;;IAErD;IACA6B,cAAc,CAACH,SAAS,CAAC,CAAC,EAAE,CAAC,EAAEE,aAAa,CAACvB,KAAK,EAAEuB,aAAa,CAACtB,MAAM,CAAC;;IAEzE;IACAuB,cAAc,CAACC,SAAS,CAAC,CAAC;IAC1BD,cAAc,CAACE,MAAM,CAAC,EAAE,EAAE,CAAC,CAAC;IAC5BF,cAAc,CAACG,MAAM,CAAC,EAAE,EAAEJ,aAAa,CAACtB,MAAM,CAAC;IAC/CuB,cAAc,CAACI,WAAW,GAAG,KAAK;IAClCJ,cAAc,CAACK,SAAS,GAAG,CAAC;IAC5BL,cAAc,CAACM,MAAM,CAAC,CAAC;;IAEvB;IACAN,cAAc,CAACO,IAAI,GAAG,YAAY;IAClCP,cAAc,CAACQ,SAAS,GAAG,MAAM;IACjCR,cAAc,CAACS,QAAQ,CAAC,iBAAiB,EAAE,GAAG,EAAE,GAAG,CAAC,CAAC,CAAC;EACxD,CAAC;EAEH,MAAMjB,qBAAqB,GAAGA,CAACF,GAAG,EAAEoB,sBAAsB,KAAK;IAC7D,IAAI3E,SAAS,EAAE;IACf;;IAEA;IACA,IAAIE,WAAW,KAAK,CAAC,IAAIqD,GAAG,CAACqB,QAAQ,CAAC,CAAC,CAAC,IAAIrB,GAAG,CAACqB,QAAQ,CAAC,CAAC,CAAC,EAAE;MAC3DjD,OAAO,CAACkD,GAAG,CAAC,6CAA6C,CAAC;MAC1D1E,cAAc,CAACD,WAAW,GAAG,CAAC,CAAC;MAC/B4E,aAAa,CAAC,CAAC;MACf;IACF;IAEA,IAAI5E,WAAW,KAAK,CAAC,IAAIqD,GAAG,CAACqB,QAAQ,CAAC,CAAC,CAAC,IAAIrB,GAAG,CAACqB,QAAQ,CAAC,CAAC,CAAC,EAAE;MAC3DjD,OAAO,CAACkD,GAAG,CAAC,6CAA6C,CAAC;MAC1D1E,cAAc,CAACD,WAAW,GAAG,CAAC,CAAC;MAC/B4E,aAAa,CAAC,CAAC;MACf;IACF;;IAEA;IACA,IAAI5E,WAAW,KAAK,CAAC,IAAIqD,GAAG,CAACqB,QAAQ,CAAC,CAAC,CAAC,IAAI,CAACrB,GAAG,CAACqB,QAAQ,CAAC,CAAC,CAAC,EAAE;MAC5DjD,OAAO,CAACkD,GAAG,CAAC,wDAAwD,CAAC;MACrE;MACA1E,cAAc,CAACD,WAAW,GAAG,CAAC,CAAC;MAC/B4E,aAAa,CAAC,CAAC;MACf;IACF;;IAEA;IACA,IAAI5E,WAAW,KAAK,CAAC,IAAIqD,GAAG,CAACqB,QAAQ,CAAC,CAAC,CAAC,IAAIrB,GAAG,CAACqB,QAAQ,CAAC,CAAC,CAAC,EAAE;MAC3DjD,OAAO,CAACkD,GAAG,CAAC,6CAA6C,CAAC;MAC1D;MACA1E,cAAc,CAACD,WAAW,GAAG,CAAC,CAAC;MAC/B4E,aAAa,CAAC,CAAC;MACf;IACF;;IAEA;IACA,IAAI5E,WAAW,KAAK,CAAC,IAAIqD,GAAG,CAACqB,QAAQ,CAAC,CAAC,CAAC,IAAIrB,GAAG,CAACqB,QAAQ,CAAC,CAAC,CAAC,EAAE;MAC3DjD,OAAO,CAACkD,GAAG,CAAC,6CAA6C,CAAC;MAC1D;MACA1E,cAAc,CAACD,WAAW,GAAG,CAAC,CAAC;MAC/B4E,aAAa,CAAC,CAAC;MACf;IACF;;IAEA;IACA,IAAI5E,WAAW,KAAK,EAAE,IAAIqD,GAAG,CAACqB,QAAQ,CAAC,CAAC,CAAC,IAAIrB,GAAG,CAACqB,QAAQ,CAAC,CAAC,CAAC,EAAE;MAC5DjD,OAAO,CAACkD,GAAG,CAAC,8CAA8C,CAAC;MAC3D;MACA1E,cAAc,CAACD,WAAW,GAAG,CAAC,CAAC;MAC/B4E,aAAa,CAAC,CAAC;MACf;IACF;IAEA,MAAMC,gBAAgB,GAAG7E,WAAW,CAAC,CAAC;;IAEtC,IAAIqD,GAAG,CAACqB,QAAQ,CAACG,gBAAgB,CAAC,EAAE;MAClCpD,OAAO,CAACkD,GAAG,CAAC,QAAQE,gBAAgB,6BAA6BA,gBAAgB,EAAE,CAAC;MACpF5E,cAAc,CAACD,WAAW,GAAG,CAAC,CAAC;MAC/B4E,aAAa,CAAC,CAAC;IACjB;EACF,CAAC;EAED,MAAMA,aAAa,GAAGA,CAAA,KAAM;IAC1B,IAAI5E,WAAW,GAAG,CAAC,IAAIU,SAAS,CAACI,OAAO,CAACgE,MAAM,EAAE,OAAO,CAAC;;IAEzD,MAAMC,YAAY,GAAGrE,SAAS,CAACI,OAAO,CAACd,WAAW,GAAG,CAAC,CAAC;IAEvDK,iBAAiB,CAAC,IAAI,CAAC,CAAC,CAAC;IACzB0E,YAAY,CAACzD,IAAI,CAAC,CAAC,CAACC,KAAK,CAACC,GAAG,IAAIC,OAAO,CAACC,KAAK,CAAC,mBAAmB,EAAEF,GAAG,CAAC,CAAC;IAEzEuD,YAAY,CAACC,OAAO,GAAG,MAAM;MAC3B3E,iBAAiB,CAAC,KAAK,CAAC,CAAC,CAAC;;MAE1B;MACA,IAAIL,WAAW,KAAKU,SAAS,CAACI,OAAO,CAACgE,MAAM,GAAG,CAAC,EAAE;QAChD/E,YAAY,CAAC,IAAI,CAAC;QAClBW,SAAS,CAACI,OAAO,CAACJ,SAAS,CAACI,OAAO,CAACgE,MAAM,GAAG,CAAC,CAAC,CAACxD,IAAI,CAAC,CAAC,CAACC,KAAK,CAACC,GAAG,IAAIC,OAAO,CAACC,KAAK,CAAC,mBAAmB,EAAEF,GAAG,CAAC,CAAC,CAAC,CAAC;MAChH;IACF,CAAC;EACH,CAAC;;EAED;EACApC,SAAS,CAAC,MAAM;IACd,IAAIc,iBAAiB,EAAE;MACrB,MAAM+E,QAAQ,GAAGC,WAAW,CAACnD,YAAY,EAAE,IAAI,GAAG,EAAE,CAAC,CAAC,CAAC;MACvD,OAAO,MAAMoD,aAAa,CAACF,QAAQ,CAAC;IACtC;EACF,CAAC,EAAE,CAAC/E,iBAAiB,EAAEF,WAAW,EAAEI,cAAc,CAAC,CAAC,CAAC,CAAC;;EAEtD,MAAMgF,aAAa,GAAGA,CAAA,KAAM;IAC1BjF,oBAAoB,CAAC,IAAI,CAAC;IAC1BO,SAAS,CAACI,OAAO,CAAC,CAAC,CAAC,CAACQ,IAAI,CAAC,CAAC,CAACC,KAAK,CAACC,GAAG,IAAIC,OAAO,CAACC,KAAK,CAAC,mBAAmB,EAAEF,GAAG,CAAC,CAAC;IACjFd,SAAS,CAACI,OAAO,CAAC,CAAC,CAAC,CAACkE,OAAO,GAAG,MAAM;MACnCtE,SAAS,CAACI,OAAO,CAAC,CAAC,CAAC,CAACQ,IAAI,CAAC,CAAC,CAACC,KAAK,CAACC,GAAG,IAAIC,OAAO,CAACC,KAAK,CAAC,mBAAmB,EAAEF,GAAG,CAAC,CAAC;IACnF,CAAC;IACDvB,cAAc,CAAC,CAAC,CAAC;EACnB,CAAC;EAED,oBACER,OAAA;IAAA4F,QAAA,GACG,CAACnF,iBAAiB,iBACjBT,OAAA;MAAQ6F,OAAO,EAAEF,aAAc;MAAAC,QAAA,EAAC;IAAc;MAAAE,QAAA,EAAAC,YAAA;MAAAC,UAAA;MAAAC,YAAA;IAAA,OAAQ,CACvD,EACAxF,iBAAiB,iBAChBT,OAAA;MAAA4F,QAAA,gBACE5F,OAAA;QAAOkG,GAAG,EAAErF,QAAS;QAACsF,KAAK,EAAE;UAAEC,OAAO,EAAE;QAAO;MAAE;QAAAN,QAAA,EAAAC,YAAA;QAAAC,UAAA;QAAAC,YAAA;MAAA,OAAE,CAAC,eACpDjG,OAAA;QAAQkG,GAAG,EAAEpF,SAAU;QAACgC,KAAK,EAAC,KAAK;QAACC,MAAM,EAAC,KAAK;QAACoD,KAAK,EAAE;UAAEE,MAAM,EAAE;QAAkB;MAAE;QAAAP,QAAA,EAAAC,YAAA;QAAAC,UAAA;QAAAC,YAAA;MAAA,OAAE,CAAC,eACzFjG,OAAA;QAAGmG,KAAK,EAAE;UAAEG,SAAS,EAAE;QAAO,CAAE;QAAAV,QAAA,EAC7BzE,cAAc,CAACZ,WAAW,GAAG,CAAC,CAAC,IAAI;MAAY;QAAAuF,QAAA,EAAAC,YAAA;QAAAC,UAAA;QAAAC,YAAA;MAAA,OAC/C,CAAC,EACH5F,SAAS,iBAAIL,OAAA;QAAGmG,KAAK,EAAE;UAAEI,KAAK,EAAE,MAAM;UAAED,SAAS,EAAE;QAAO,CAAE;QAAAV,QAAA,EAAC;MAA0B;QAAAE,QAAA,EAAAC,YAAA;QAAAC,UAAA;QAAAC,YAAA;MAAA,OAAG,CAAC;IAAA;MAAAH,QAAA,EAAAC,YAAA;MAAAC,UAAA;MAAAC,YAAA;IAAA,OACzF,CACN;EAAA;IAAAH,QAAA,EAAAC,YAAA;IAAAC,UAAA;IAAAC,YAAA;EAAA,OACE,CAAC;AAEV,CAAC;AAAC/F,EAAA,CAxWID,UAAU;AAAAuG,EAAA,GAAVvG,UAAU;AA0WhB,eAAeA,UAAU;AAAC,IAAAuG,EAAA;AAAAC,YAAA,CAAAD,EAAA","ignoreList":[]},"metadata":{},"sourceType":"module","externalDependencies":[]}