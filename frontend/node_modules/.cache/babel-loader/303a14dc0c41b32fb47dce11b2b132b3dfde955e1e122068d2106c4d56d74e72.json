{"ast":null,"code":"var _jsxFileName = \"/home/user/Documents/Aruco_POC/aruco-detector/src/CameraFeed.js\",\n  _s = $RefreshSig$();\n// import React, { useEffect, useRef, useState } from 'react';\n// import axios from 'axios';\n\n// const CameraFeed = () => {\n//   const [detected, setDetected] = useState(false)  \n//   const videoRef = useRef(null);\n//   const canvasRef = useRef(null);\n//   const [processing, setProcessing] = useState(false);\n\n//   useEffect(() => {\n//     const video = videoRef.current;\n\n//     // Request access to the webcam\n//     navigator.mediaDevices.getUserMedia({ video: true })\n//       .then((stream) => {\n//         video.srcObject = stream;\n\n//         // Only call play() when the video is loaded\n//         video.onloadedmetadata = () => {\n//           video.play().catch(error => console.error(\"Error playing video:\", error));\n//         };\n//       })\n//       .catch(err => console.error(\"Error accessing webcam:\", err));\n\n//     return () => {\n//       if (video.srcObject) {\n//         video.srcObject.getTracks().forEach(track => track.stop()); // Stop webcam access when the component unmounts\n//       }\n//     };\n//   }, []);\n\n//   let lastProcessedTime = Date.now();\n\n//   const processFrame = async () => {\n//     const currentTime = Date.now();\n//     const timeDiff = currentTime - lastProcessedTime;\n\n//     // Adjust frame rate by only sending frames every 200ms (5 FPS, adjust as necessary)\n//     if (timeDiff < 200) {\n//       return;\n//     }\n\n//     lastProcessedTime = currentTime;\n\n//     const canvas = canvasRef.current;\n//     const bufferCanvas = document.createElement('canvas');  // Create an off-screen buffer canvas\n//     bufferCanvas.width = canvas.width;\n//     bufferCanvas.height = canvas.height;\n\n//     const context = canvas.getContext('2d');\n//     const bufferContext = bufferCanvas.getContext('2d');  // Context for the off-screen canvas\n\n//     // Draw the current frame from video on buffer canvas\n//     bufferContext.drawImage(videoRef.current, 0, 0, bufferCanvas.width, bufferCanvas.height);\n\n//     // Convert the frame to Blob and send to the backend for processing\n//     bufferCanvas.toBlob(async (blob) => {\n//       const formData = new FormData();\n//       formData.append('image', blob, 'frame.jpg');\n\n//       try {\n//         setProcessing(true);\n//         const response = await axios.post('http://localhost:5000/detect', formData, {\n//           responseType: 'blob'\n//         });\n\n//         // Create an image object from the returned blob\n//         const img = new Image();\n//         img.src = URL.createObjectURL(response.data);\n//         img.onload = () => {\n//           context.clearRect(0, 0, canvas.width, canvas.height);  // Clear the previous frame\n//           context.drawImage(img, 0, 0, canvas.width, canvas.height);  // Draw the augmented frame\n//           setProcessing(false);\n//           setDetected(true)\n//         };\n\n//       } catch (err) {\n//         console.error(\"Error processing frame:\", err);\n//         setProcessing(false);\n//       }\n//     }, 'image/jpeg');\n//   };\n\n//   useEffect(() => {\n//     const interval = setInterval(processFrame, 1000 / 10); // Process 10 frames per second\n//     return () => clearInterval(interval);\n//   }, []);\n\n//   return (\n//     <div>\n//       <video ref={videoRef} style={{ display: 'none' }} />\n//       <canvas ref={canvasRef} width=\"640\" height=\"480\" />\n//       {/* {processing } */}\n//     {/* Display the text \"Chemical pouring\" when marker is detected */}\n//     {detected && <p style={{ color: 'green', marginTop: '10px' }}>Chemical pouring</p>}\n//     </div>\n//   );\n// };\n\n// export default CameraFeed;\n\n// import React, { useEffect, useRef, useState } from 'react';\n// import axios from 'axios';\n\n// const CameraFeed = () => {\n//   const [detected, setDetected] = useState(false);\n//   const videoRef = useRef(null);\n//   const canvasRef = useRef(null);\n//   const [processing, setProcessing] = useState(false);\n\n//   useEffect(() => {\n//     const video = videoRef.current;\n\n//     // Request access to the webcam\n//     navigator.mediaDevices.getUserMedia({ video: true })\n//       .then((stream) => {\n//         video.srcObject = stream;\n\n//         // Only call play() when the video is loaded\n//         video.onloadedmetadata = () => {\n//           video.play().catch(error => console.error(\"Error playing video:\", error));\n//         };\n//       })\n//       .catch(err => console.error(\"Error accessing webcam:\", err));\n\n//     return () => {\n//       if (video.srcObject) {\n//         video.srcObject.getTracks().forEach(track => track.stop()); // Stop webcam access when the component unmounts\n//       }\n//     };\n//   }, []);\n\n//   let lastProcessedTime = Date.now();\n\n//   const processFrame = async () => {\n//     const currentTime = Date.now();\n//     const timeDiff = currentTime - lastProcessedTime;\n\n//     // Adjust frame rate by only sending frames every 200ms (5 FPS, adjust as necessary)\n//     if (timeDiff < 200) {\n//       return;\n//     }\n\n//     lastProcessedTime = currentTime;\n\n//     const canvas = canvasRef.current;\n//     const bufferCanvas = document.createElement('canvas');  // Create an off-screen buffer canvas\n//     bufferCanvas.width = canvas.width;\n//     bufferCanvas.height = canvas.height;\n\n//     const context = canvas.getContext('2d');\n//     const bufferContext = bufferCanvas.getContext('2d');  // Context for the off-screen canvas\n\n//     // Draw the current frame from video on buffer canvas\n//     bufferContext.drawImage(videoRef.current, 0, 0, bufferCanvas.width, bufferCanvas.height);\n\n//     // Convert the frame to Blob and send to the backend for processing\n//     bufferCanvas.toBlob(async (blob) => {\n//       const formData = new FormData();\n//       formData.append('image', blob, 'frame.jpg');\n\n//       try {\n//         setProcessing(true);\n//         const response = await axios.post('http://localhost:5000/detect', formData, {\n//           responseType: 'json'\n//         });\n\n//         // Handle the response\n//         const { detected, augmented_image } = response.data;\n\n//         // Create an image object from the base64 encoded image\n//         const img = new Image();\n//         img.src = `data:image/jpeg;base64,${augmented_image}`;\n//         img.onload = () => {\n//           const canvasContext = canvas.getContext('2d');\n//           canvasContext.drawImage(img, 0, 0, canvas.width, canvas.height);  // Draw the augmented frame\n//           setProcessing(false);\n//           setDetected(detected);  // Set detected based on the backend response\n//         };\n\n//       } catch (err) {\n//         console.error(\"Error processing frame:\", err);\n//         setProcessing(false);\n//         setDetected(false);  // Ensure detection state is false if an error occurs\n//       }\n//     }, 'image/jpeg');\n//   };\n\n//   useEffect(() => {\n//     const interval = setInterval(processFrame, 1000 / 10); // Process 10 frames per second\n//     return () => clearInterval(interval);\n//   }, []);\n\n//   return (\n//     <div>\n//       <video ref={videoRef} style={{ display: 'none' }} />\n//       <canvas ref={canvasRef} width=\"640\" height=\"480\" />\n//       {/* Display the text \"Chemical pouring\" only when marker is detected */}\n//       {detected && <p style={{ color: 'green', marginTop: '10px' }}>Chemical pouring</p>}\n//     </div>\n//   );\n// };\n\n// export default CameraFeed;\n\n//**********WORKING PLAY AUDIO THEN TAKE DETECTION**************** */\n\n// import React, { useEffect, useRef, useState } from 'react';\n// import axios from 'axios';\n\n// const CameraFeed = () => {\n//   const [detected, setDetected] = useState(false);\n//   const [tutorialStarted, setTutorialStarted] = useState(false);\n//   const videoRef = useRef(null);\n//   const canvasRef = useRef(null);\n//   const [processing, setProcessing] = useState(false);\n//   const audioRef = useRef(new Audio('/distilled_water.mp3'));\n\n//   const startTutorial = () => {\n//     audioRef.current.play().then(() => {\n//       setTutorialStarted(true); // Start camera feed after audio plays\n//     }).catch(err => {\n//       console.error(\"Audio play error:\", err);\n//     });\n//   };\n\n//   useEffect(() => {\n//     if (tutorialStarted) {\n//       const video = videoRef.current;\n\n//       // Request access to the webcam\n//       navigator.mediaDevices.getUserMedia({ video: true })\n//         .then((stream) => {\n//           video.srcObject = stream;\n\n//           // Only call play() when the video is loaded\n//           video.onloadedmetadata = () => {\n//             video.play().catch(error => console.error(\"Error playing video:\", error));\n//           };\n//         })\n//         .catch(err => console.error(\"Error accessing webcam:\", err));\n\n//       return () => {\n//         if (video.srcObject) {\n//           video.srcObject.getTracks().forEach(track => track.stop()); // Stop webcam access when the component unmounts\n//         }\n//       };\n//     }\n//   }, [tutorialStarted]);\n\n//   let lastProcessedTime = Date.now();\n\n//   const processFrame = async () => {\n//     const currentTime = Date.now();\n//     const timeDiff = currentTime - lastProcessedTime;\n\n//     // Adjust frame rate by only sending frames every 200ms (5 FPS, adjust as necessary)\n//     if (timeDiff < 200) {\n//       return;\n//     }\n\n//     lastProcessedTime = currentTime;\n\n//     const canvas = canvasRef.current;\n//     const bufferCanvas = document.createElement('canvas');  // Create an off-screen buffer canvas\n//     bufferCanvas.width = canvas.width;\n//     bufferCanvas.height = canvas.height;\n\n//     const context = canvas.getContext('2d');\n//     const bufferContext = bufferCanvas.getContext('2d');  // Context for the off-screen canvas\n\n//     // Draw the current frame from video on buffer canvas\n//     bufferContext.drawImage(videoRef.current, 0, 0, bufferCanvas.width, bufferCanvas.height);\n\n//     // Convert the frame to Blob and send to the backend for processing\n//     bufferCanvas.toBlob(async (blob) => {\n//       const formData = new FormData();\n//       formData.append('image', blob, 'frame.jpg');\n\n//       try {\n//         setProcessing(true);\n//         const response = await axios.post('http://localhost:5000/detect', formData, {\n//           responseType: 'json'\n//         });\n\n//         // Handle the response\n//         const { detected, augmented_image } = response.data;\n\n//         // Create an image object from the base64 encoded image\n//         const img = new Image();\n//         img.src = `data:image/jpeg;base64,${augmented_image}`;\n//         img.onload = () => {\n//           const canvasContext = canvas.getContext('2d');\n//           canvasContext.drawImage(img, 0, 0, canvas.width, canvas.height);  // Draw the augmented frame\n//           setProcessing(false);\n//           setDetected(detected);  // Set detected based on the backend response\n//         };\n\n//       } catch (err) {\n//         console.error(\"Error processing frame:\", err);\n//         setProcessing(false);\n//         setDetected(false);  // Ensure detection state is false if an error occurs\n//       }\n//     }, 'image/jpeg');\n//   };\n\n//   useEffect(() => {\n//     if (tutorialStarted) {\n//       const interval = setInterval(processFrame, 1000 / 10); // Process 10 frames per second\n//       return () => clearInterval(interval);\n//     }\n//   }, [tutorialStarted]);\n\n//   return (\n//     <div>\n//       {!tutorialStarted && <button onClick={startTutorial}>Start Tutorial</button>}\n//       <video ref={videoRef} style={{ display: 'none' }} />\n//       <canvas ref={canvasRef} width=\"640\" height=\"480\" />\n//       {/* Display the text \"Chemical pouring\" only when marker with ID 0 is detected */}\n//       {/* {detected && <p style={{ color: 'green', marginTop: '10px' }}>Chemical pouring</p>} */}\n//       {/* Display \"Completed\" when marker with ID 0 is detected */}\n//       {detected && <p style={{ color: 'blue', marginTop: '10px' }}>Completed</p>}\n//     </div>\n//   );\n// };\n\n// export default CameraFeed;\n\n// import React, { useEffect, useRef, useState } from 'react';\n// import axios from 'axios';\n\n// const CameraFeed = () => {\n//   const [detected, setDetected] = useState(false);\n//   const [completed, setCompleted] = useState(false);\n//   const videoRef = useRef(null);\n//   const canvasRef = useRef(null);\n//   const [processing, setProcessing] = useState(false);\n//   const audioRef = useRef(new Audio('/voice1.mp3'));\n//   const completionAudioRef = useRef(new Audio('/completed.mp3'));\n\n//   const startTutorial = () => {\n//     audioRef.current.play().catch(err => console.error(\"Audio play error:\", err));\n//   };\n\n//   useEffect(() => {\n//     const video = videoRef.current;\n\n//     // Request access to the webcam\n//     navigator.mediaDevices.getUserMedia({ video: true })\n//       .then((stream) => {\n//         video.srcObject = stream;\n\n//         // Only call play() when the video is loaded\n//         video.onloadedmetadata = () => {\n//           video.play().catch(error => console.error(\"Error playing video:\", error));\n//         };\n//       })\n//       .catch(err => console.error(\"Error accessing webcam:\", err));\n\n//     return () => {\n//       if (video.srcObject) {\n//         video.srcObject.getTracks().forEach(track => track.stop()); // Stop webcam access when the component unmounts\n//       }\n//     };\n//   }, []);\n\n//   let lastProcessedTime = Date.now();\n\n//   const processFrame = async () => {\n//     if (completed) return; // Stop processing if completed\n\n//     const currentTime = Date.now();\n//     const timeDiff = currentTime - lastProcessedTime;\n\n//     // Adjust frame rate by only sending frames every 200ms (5 FPS, adjust as necessary)\n//     if (timeDiff < 200) {\n//       return;\n//     }\n\n//     lastProcessedTime = currentTime;\n\n//     const canvas = canvasRef.current;\n//     const bufferCanvas = document.createElement('canvas');  // Create an off-screen buffer canvas\n//     bufferCanvas.width = canvas.width;\n//     bufferCanvas.height = canvas.height;\n\n//     const context = canvas.getContext('2d');\n//     const bufferContext = bufferCanvas.getContext('2d');  // Context for the off-screen canvas\n\n//     // Draw the current frame from video on buffer canvas\n//     bufferContext.drawImage(videoRef.current, 0, 0, bufferCanvas.width, bufferCanvas.height);\n\n//     // Convert the frame to Blob and send to the backend for processing\n//     bufferCanvas.toBlob(async (blob) => {\n//       const formData = new FormData();\n//       formData.append('image', blob, 'frame.jpg');\n\n//       try {\n//         setProcessing(true);\n//         const response = await axios.post('http://localhost:5000/detect', formData, {\n//           responseType: 'json'\n//         });\n\n//         console.log(\"Response Data:\", response.data); // Log the entire response data\n\n//         // Handle the response\n//         const { detected, augmented_image, ids } = response.data;\n\n//         // Check if detection happened\n//         if (detected) {\n//           console.log('Detection is true');\n//           console.log('Detected IDs:', ids); // This should now log the detected IDs\n\n//           // Check if the detected marker is ID 0\n//           if (ids && ids.includes(0)) {\n//             setCompleted(true);\n//             console.log('Marker ID 0 detected');\n//             videoRef.current.srcObject.getTracks().forEach(track => track.stop()); // Pause camera stream\n//             completionAudioRef.current.play().catch(err => console.error(\"Completion audio play error:\", err));\n//           }\n//         }\n\n//         // Create an image object from the base64 encoded image\n//         const img = new Image();\n//         img.src = `data:image/jpeg;base64,${augmented_image}`;\n//         img.onload = () => {\n//           const canvasContext = canvas.getContext('2d');\n//           canvasContext.drawImage(img, 0, 0, canvas.width, canvas.height);  // Draw the augmented frame\n//           setProcessing(false);\n//           setDetected(detected);  // Set detected based on the backend response\n//         };\n\n//       } catch (err) {\n//         console.error(\"Error processing frame:\", err);\n//         setProcessing(false);\n//         setDetected(false);  // Ensure detection state is false if an error occurs\n//       }\n//     }, 'image/jpeg');\n//   };\n\n//   useEffect(() => {\n//     const interval = setInterval(processFrame, 1000 / 10); // Process 10 frames per second\n//     return () => clearInterval(interval);\n//   }, [completed]);\n\n//   return (\n//     <div>\n//       <button onClick={startTutorial}>Start Tutorial</button>\n//       <video ref={videoRef} style={{ display: 'none' }} />\n//       <canvas ref={canvasRef} width=\"640\" height=\"480\" />\n//       {/* Display the text \"Chemical pouring\" only when marker is detected */}\n//       {/* {detected && <p style={{ color: 'green', marginTop: '10px' }}>Chemical pouring</p>} */}\n//       {completed && <p style={{ color: 'blue', marginTop: '10px' }}>Completed</p>}\n//     </div>\n//   );\n// };\n\n// export default CameraFeed;\n\n// import React, { useEffect, useRef, useState } from 'react';\n// import axios from 'axios';\n\n// const CameraFeed = () => {\n//   const [detected, setDetected] = useState(false);\n//   const [completed, setCompleted] = useState(false);\n//   const [currentStep, setCurrentStep] = useState(0);  // Track the current step\n//   const videoRef = useRef(null);\n//   const canvasRef = useRef(null);\n//   const audioRefs = useRef([\n//     new Audio('/voice1.mp3'), // Step 1: Introduction to sulfate test\n//     new Audio('/voice2.mp3'), // Step 2: Show distilled water\n//     new Audio('/voice3.mp3'), // Step 3: Show test tube 1\n//     // Add the rest of the steps here\n//     new Audio('/voice13.mp3'), // Step 13: Test completion message\n//   ]);\n\n//   useEffect(() => {\n//     // Play the first step's audio when the component is loaded\n//     audioRefs.current[0].play().catch(err => console.error(\"Audio play error:\", err));\n\n//     // Web camera initialization logic\n//     const video = videoRef.current;\n//     navigator.mediaDevices.getUserMedia({ video: true })\n//       .then((stream) => {\n//         video.srcObject = stream;\n//         video.onloadedmetadata = () => {\n//           video.play().catch(error => console.error(\"Error playing video:\", error));\n//         };\n//       })\n//       .catch(err => console.error(\"Error accessing webcam:\", err));\n\n//     return () => {\n//       if (video.srcObject) {\n//         video.srcObject.getTracks().forEach(track => track.stop());\n//       }\n//     };\n//   }, []);\n\n//   const processFrame = async () => {\n//     if (completed) return;  // Stop processing if test is complete\n\n//     const canvas = canvasRef.current;\n//     const context = canvas.getContext('2d');\n//     context.drawImage(videoRef.current, 0, 0, canvas.width, canvas.height);\n\n//     // Send frame to backend for ArUco detection\n//     canvas.toBlob(async (blob) => {\n//       const formData = new FormData();\n//       formData.append('image', blob, 'frame.jpg');\n//       try {\n//         const response = await axios.post('http://localhost:5000/detect', formData, { responseType: 'json' });\n//         const { detected, ids } = response.data;\n\n//         if (detected && ids) {\n//           handleMarkerDetection(ids);\n//         }\n\n//         // Display processed frame (augmented) coming from backend\n//         const img = new Image();\n//         img.src = `data:image/jpeg;base64,${response.data.augmented_image}`;\n//         img.onload = () => context.drawImage(img, 0, 0, canvas.width, canvas.height);\n//       } catch (err) {\n//         console.error(\"Error processing frame:\", err);\n//       }\n//     }, 'image/jpeg');\n//   };\n\n//   const handleMarkerDetection = (ids) => {\n//     const expectedMarkerId = currentStep;  // Assuming marker ID matches the current step\n//     if (ids.includes(expectedMarkerId)) {\n//       console.log(`Step ${currentStep} verified with marker ID: ${expectedMarkerId}`);\n\n//       // Move to the next step\n//       setCurrentStep(currentStep + 1);\n\n//       // Stop current audio and play the next\n//       if (currentStep < audioRefs.current.length) {\n//         audioRefs.current[currentStep].pause();\n//         audioRefs.current[currentStep + 1].play().catch(err => console.error(\"Audio play error:\", err));\n//       }\n\n//       // Mark test as completed when last step is done\n//       if (currentStep === audioRefs.current.length - 1) {\n//         setCompleted(true);\n//       }\n//     }\n//   };\n\n//   useEffect(() => {\n//     const interval = setInterval(processFrame, 1000 / 10);  // 10 FPS\n//     return () => clearInterval(interval);\n//   }, [currentStep]);\n\n//   return (\n//     <div>\n//       <video ref={videoRef} style={{ display: 'none' }} />\n//       <canvas ref={canvasRef} width=\"640\" height=\"480\" />\n//       {completed && <p style={{ color: 'blue', marginTop: '10px' }}>Sulfate Ion Test Completed</p>}\n//     </div>\n//   );\n// };\n\n// export default CameraFeed;\n\nimport React, { useEffect, useRef, useState } from 'react';\nimport axios from 'axios';\nimport { jsxDEV as _jsxDEV } from \"react/jsx-dev-runtime\";\nconst CameraFeed = () => {\n  _s();\n  const [detected, setDetected] = useState(false);\n  const [completed, setCompleted] = useState(false);\n  const [currentStep, setCurrentStep] = useState(0); // Track the current step\n  const [isTutorialStarted, setIsTutorialStarted] = useState(false); // Track whether tutorial has started\n  const videoRef = useRef(null);\n  const canvasRef = useRef(null);\n  // const audioRefs = useRef([\n  //   new Audio('/voice1.mp3'), // Step 1: Introduction to sulfate test\n  //   new Audio('/voice2.mp3'), // Step 2: Show distilled water\n  //   new Audio('/voice3.mp3'), // Step 3: Show test tube 1\n  //   // new Audio('/voice4.mp3'),\n  //   // Add the rest of the steps here\n  //   new Audio('/voice13.mp3'), // Step 13: Test completion message\n  // ]);\n  const audioRefs = useRef([['/voice1.mp3'],\n  // Step 1: Introduction to sulfate test\n  ['/voice2.mp3'],\n  // Step 2: Show distilled water\n  ['/voice3.mp3'],\n  // Step 3: Show test tube 1\n  // Add more audio arrays as needed\n  ['/voice13.mp3'] // Step 13: Test completion message\n  ]);\n  const playAudios = (audios, index = 0) => {\n    if (index < audios.length) {\n      const audio = new Audio(audios[index]);\n      audio.play().catch(err => console.error(\"Audio play error:\", err));\n      audio.onended = () => playAudios(audios, index + 1); // Play next audio when current one ends\n    }\n  };\n\n  // Start camera feed and process frames only when the tutorial is started\n  useEffect(() => {\n    if (isTutorialStarted) {\n      const video = videoRef.current;\n\n      // Web camera initialization logic\n      navigator.mediaDevices.getUserMedia({\n        video: true\n      }).then(stream => {\n        video.srcObject = stream;\n        video.onloadedmetadata = () => {\n          video.play();\n        };\n      }).catch(err => console.error(\"Error accessing webcam:\", err));\n\n      // Clean up: stop the camera stream when component unmounts\n      return () => {\n        if (video.srcObject) {\n          video.srcObject.getTracks().forEach(track => track.stop());\n        }\n      };\n    }\n  }, [isTutorialStarted]); // Only run this effect when the tutorial starts\n\n  const processFrame = async () => {\n    if (!isTutorialStarted || completed) return; // Stop processing if the tutorial hasn't started or test is complete\n\n    const canvas = canvasRef.current;\n    const context = canvas.getContext('2d');\n\n    // Ensure video is ready and draw the video feed onto the canvas\n    const video = videoRef.current;\n    if (video.readyState === video.HAVE_ENOUGH_DATA) {\n      context.drawImage(video, 0, 0, canvas.width, canvas.height);\n\n      // Send frame to backend for ArUco detection\n      canvas.toBlob(async blob => {\n        const formData = new FormData();\n        formData.append('image', blob, 'frame.jpg');\n        try {\n          const response = await axios.post('http://localhost:5000/detect', formData, {\n            responseType: 'json'\n          });\n          console.log(\"Response from backend:\", response.data); // Log response\n\n          const {\n            detected,\n            ids,\n            augmented_image\n          } = response.data; // Destructure response\n          console.log(`Detected: ${detected}, IDs: ${ids}, Augmented Image Length: ${augmented_image.length}`); // Log image length\n\n          if (detected && ids) {\n            handleMarkerDetection(ids);\n          }\n\n          // Clear the canvas before drawing the new image\n          context.clearRect(0, 0, canvas.width, canvas.height);\n\n          // Display processed frame (augmented) coming from backend\n          const img = new Image();\n          img.src = `data:image/jpeg;base64,${augmented_image}`;\n          img.onload = () => {\n            context.drawImage(img, 0, 0, canvas.width, canvas.height);\n          };\n        } catch (err) {\n          console.error(\"Error processing frame:\", err);\n        }\n      }, 'image/jpeg');\n    }\n  };\n  const handleMarkerDetection = ids => {\n    const expectedMarkerId = currentStep; // Assuming marker ID matches the current step\n    if (ids.includes(expectedMarkerId)) {\n      console.log(`Step ${currentStep} verified with marker ID: ${expectedMarkerId}`);\n\n      // Move to the next step\n      setCurrentStep(currentStep + 1);\n\n      // Stop current audio and play the next\n      if (currentStep < audioRefs.current.length) {\n        console.log('-----------------------------------', currentStep);\n        audioRefs.current[currentStep].pause();\n        audioRefs.current[currentStep + 1].play().catch(err => console.error(\"Audio play error:\", err));\n      }\n\n      // Mark test as completed when last step is done\n      if (currentStep === audioRefs.current.length - 1) {\n        setCompleted(true);\n      }\n    }\n  };\n\n  // Trigger the frame processing when the tutorial starts\n  useEffect(() => {\n    if (isTutorialStarted) {\n      const interval = setInterval(processFrame, 1000 / 10); // 10 FPS\n      return () => clearInterval(interval);\n    }\n  }, [isTutorialStarted, currentStep]);\n  const startTutorial = () => {\n    setIsTutorialStarted(true);\n    // audioRefs.current[0].play().catch(err => console.error(\"Audio play error:\", err));\n    playAudios(audioRefs.current[step]); // Play audio for the specific step\n  };\n\n  // const startTutorial = () => {\n  //   setIsTutorialStarted(true);\n\n  //   // Play the first audio\n  //   audioRefs.current[0].play().catch(err => console.error(\"Audio play error:\", err));\n\n  //   // Set up an event listener to play the second audio after the first one ends\n  //   audioRefs.current[0].onended = () => {\n  //     audioRefs.current[1].play().catch(err => console.error(\"Audio play error:\", err));\n  //   };\n  // };\n\n  return /*#__PURE__*/_jsxDEV(\"div\", {\n    children: [!isTutorialStarted && /*#__PURE__*/_jsxDEV(\"button\", {\n      onClick: startTutorial,\n      children: \"Start Tutorial\"\n    }, void 0, false, {\n      fileName: _jsxFileName,\n      lineNumber: 743,\n      columnNumber: 9\n    }, this), isTutorialStarted && /*#__PURE__*/_jsxDEV(\"div\", {\n      children: [/*#__PURE__*/_jsxDEV(\"video\", {\n        ref: videoRef,\n        style: {\n          display: 'none'\n        }\n      }, void 0, false, {\n        fileName: _jsxFileName,\n        lineNumber: 747,\n        columnNumber: 11\n      }, this), /*#__PURE__*/_jsxDEV(\"canvas\", {\n        ref: canvasRef,\n        width: \"640\",\n        height: \"480\",\n        style: {\n          border: '1px solid black'\n        }\n      }, void 0, false, {\n        fileName: _jsxFileName,\n        lineNumber: 748,\n        columnNumber: 11\n      }, this), completed && /*#__PURE__*/_jsxDEV(\"p\", {\n        style: {\n          color: 'blue',\n          marginTop: '10px'\n        },\n        children: \"Sulfate Ion Test Completed\"\n      }, void 0, false, {\n        fileName: _jsxFileName,\n        lineNumber: 749,\n        columnNumber: 25\n      }, this)]\n    }, void 0, true, {\n      fileName: _jsxFileName,\n      lineNumber: 746,\n      columnNumber: 9\n    }, this)]\n  }, void 0, true, {\n    fileName: _jsxFileName,\n    lineNumber: 741,\n    columnNumber: 5\n  }, this);\n};\n_s(CameraFeed, \"BJLpCL0sha16shpHbVedWYJC9xk=\");\n_c = CameraFeed;\nexport default CameraFeed;\nvar _c;\n$RefreshReg$(_c, \"CameraFeed\");","map":{"version":3,"names":["React","useEffect","useRef","useState","axios","jsxDEV","_jsxDEV","CameraFeed","_s","detected","setDetected","completed","setCompleted","currentStep","setCurrentStep","isTutorialStarted","setIsTutorialStarted","videoRef","canvasRef","audioRefs","playAudios","audios","index","length","audio","Audio","play","catch","err","console","error","onended","video","current","navigator","mediaDevices","getUserMedia","then","stream","srcObject","onloadedmetadata","getTracks","forEach","track","stop","processFrame","canvas","context","getContext","readyState","HAVE_ENOUGH_DATA","drawImage","width","height","toBlob","blob","formData","FormData","append","response","post","responseType","log","data","ids","augmented_image","handleMarkerDetection","clearRect","img","Image","src","onload","expectedMarkerId","includes","pause","interval","setInterval","clearInterval","startTutorial","step","children","onClick","fileName","_jsxFileName","lineNumber","columnNumber","ref","style","display","border","color","marginTop","_c","$RefreshReg$"],"sources":["/home/user/Documents/Aruco_POC/aruco-detector/src/CameraFeed.js"],"sourcesContent":["// import React, { useEffect, useRef, useState } from 'react';\n// import axios from 'axios';\n\n// const CameraFeed = () => {\n//   const [detected, setDetected] = useState(false)  \n//   const videoRef = useRef(null);\n//   const canvasRef = useRef(null);\n//   const [processing, setProcessing] = useState(false);\n  \n//   useEffect(() => {\n//     const video = videoRef.current;\n  \n//     // Request access to the webcam\n//     navigator.mediaDevices.getUserMedia({ video: true })\n//       .then((stream) => {\n//         video.srcObject = stream;\n  \n//         // Only call play() when the video is loaded\n//         video.onloadedmetadata = () => {\n//           video.play().catch(error => console.error(\"Error playing video:\", error));\n//         };\n//       })\n//       .catch(err => console.error(\"Error accessing webcam:\", err));\n  \n//     return () => {\n//       if (video.srcObject) {\n//         video.srcObject.getTracks().forEach(track => track.stop()); // Stop webcam access when the component unmounts\n//       }\n//     };\n//   }, []);\n\n\n//   let lastProcessedTime = Date.now();\n\n//   const processFrame = async () => {\n//     const currentTime = Date.now();\n//     const timeDiff = currentTime - lastProcessedTime;\n  \n//     // Adjust frame rate by only sending frames every 200ms (5 FPS, adjust as necessary)\n//     if (timeDiff < 200) {\n//       return;\n//     }\n  \n//     lastProcessedTime = currentTime;\n  \n//     const canvas = canvasRef.current;\n//     const bufferCanvas = document.createElement('canvas');  // Create an off-screen buffer canvas\n//     bufferCanvas.width = canvas.width;\n//     bufferCanvas.height = canvas.height;\n  \n//     const context = canvas.getContext('2d');\n//     const bufferContext = bufferCanvas.getContext('2d');  // Context for the off-screen canvas\n    \n//     // Draw the current frame from video on buffer canvas\n//     bufferContext.drawImage(videoRef.current, 0, 0, bufferCanvas.width, bufferCanvas.height);\n    \n//     // Convert the frame to Blob and send to the backend for processing\n//     bufferCanvas.toBlob(async (blob) => {\n//       const formData = new FormData();\n//       formData.append('image', blob, 'frame.jpg');\n  \n//       try {\n//         setProcessing(true);\n//         const response = await axios.post('http://localhost:5000/detect', formData, {\n//           responseType: 'blob'\n//         });\n        \n//         // Create an image object from the returned blob\n//         const img = new Image();\n//         img.src = URL.createObjectURL(response.data);\n//         img.onload = () => {\n//           context.clearRect(0, 0, canvas.width, canvas.height);  // Clear the previous frame\n//           context.drawImage(img, 0, 0, canvas.width, canvas.height);  // Draw the augmented frame\n//           setProcessing(false);\n//           setDetected(true)\n//         };\n        \n//       } catch (err) {\n//         console.error(\"Error processing frame:\", err);\n//         setProcessing(false);\n//       }\n//     }, 'image/jpeg');\n//   };\n  \n  \n\n//   useEffect(() => {\n//     const interval = setInterval(processFrame, 1000 / 10); // Process 10 frames per second\n//     return () => clearInterval(interval);\n//   }, []);\n\n//   return (\n//     <div>\n//       <video ref={videoRef} style={{ display: 'none' }} />\n//       <canvas ref={canvasRef} width=\"640\" height=\"480\" />\n//       {/* {processing } */}\n//     {/* Display the text \"Chemical pouring\" when marker is detected */}\n//     {detected && <p style={{ color: 'green', marginTop: '10px' }}>Chemical pouring</p>}\n//     </div>\n//   );\n// };\n\n// export default CameraFeed;\n\n\n\n// import React, { useEffect, useRef, useState } from 'react';\n// import axios from 'axios';\n\n// const CameraFeed = () => {\n//   const [detected, setDetected] = useState(false);\n//   const videoRef = useRef(null);\n//   const canvasRef = useRef(null);\n//   const [processing, setProcessing] = useState(false);\n\n//   useEffect(() => {\n//     const video = videoRef.current;\n\n//     // Request access to the webcam\n//     navigator.mediaDevices.getUserMedia({ video: true })\n//       .then((stream) => {\n//         video.srcObject = stream;\n\n//         // Only call play() when the video is loaded\n//         video.onloadedmetadata = () => {\n//           video.play().catch(error => console.error(\"Error playing video:\", error));\n//         };\n//       })\n//       .catch(err => console.error(\"Error accessing webcam:\", err));\n\n//     return () => {\n//       if (video.srcObject) {\n//         video.srcObject.getTracks().forEach(track => track.stop()); // Stop webcam access when the component unmounts\n//       }\n//     };\n//   }, []);\n\n//   let lastProcessedTime = Date.now();\n\n//   const processFrame = async () => {\n//     const currentTime = Date.now();\n//     const timeDiff = currentTime - lastProcessedTime;\n\n//     // Adjust frame rate by only sending frames every 200ms (5 FPS, adjust as necessary)\n//     if (timeDiff < 200) {\n//       return;\n//     }\n\n//     lastProcessedTime = currentTime;\n\n//     const canvas = canvasRef.current;\n//     const bufferCanvas = document.createElement('canvas');  // Create an off-screen buffer canvas\n//     bufferCanvas.width = canvas.width;\n//     bufferCanvas.height = canvas.height;\n\n//     const context = canvas.getContext('2d');\n//     const bufferContext = bufferCanvas.getContext('2d');  // Context for the off-screen canvas\n\n//     // Draw the current frame from video on buffer canvas\n//     bufferContext.drawImage(videoRef.current, 0, 0, bufferCanvas.width, bufferCanvas.height);\n\n//     // Convert the frame to Blob and send to the backend for processing\n//     bufferCanvas.toBlob(async (blob) => {\n//       const formData = new FormData();\n//       formData.append('image', blob, 'frame.jpg');\n\n//       try {\n//         setProcessing(true);\n//         const response = await axios.post('http://localhost:5000/detect', formData, {\n//           responseType: 'json'\n//         });\n\n//         // Handle the response\n//         const { detected, augmented_image } = response.data;\n\n//         // Create an image object from the base64 encoded image\n//         const img = new Image();\n//         img.src = `data:image/jpeg;base64,${augmented_image}`;\n//         img.onload = () => {\n//           const canvasContext = canvas.getContext('2d');\n//           canvasContext.drawImage(img, 0, 0, canvas.width, canvas.height);  // Draw the augmented frame\n//           setProcessing(false);\n//           setDetected(detected);  // Set detected based on the backend response\n//         };\n\n//       } catch (err) {\n//         console.error(\"Error processing frame:\", err);\n//         setProcessing(false);\n//         setDetected(false);  // Ensure detection state is false if an error occurs\n//       }\n//     }, 'image/jpeg');\n//   };\n\n//   useEffect(() => {\n//     const interval = setInterval(processFrame, 1000 / 10); // Process 10 frames per second\n//     return () => clearInterval(interval);\n//   }, []);\n\n//   return (\n//     <div>\n//       <video ref={videoRef} style={{ display: 'none' }} />\n//       <canvas ref={canvasRef} width=\"640\" height=\"480\" />\n//       {/* Display the text \"Chemical pouring\" only when marker is detected */}\n//       {detected && <p style={{ color: 'green', marginTop: '10px' }}>Chemical pouring</p>}\n//     </div>\n//   );\n// };\n\n// export default CameraFeed;\n\n\n//**********WORKING PLAY AUDIO THEN TAKE DETECTION**************** */\n\n\n// import React, { useEffect, useRef, useState } from 'react';\n// import axios from 'axios';\n\n// const CameraFeed = () => {\n//   const [detected, setDetected] = useState(false);\n//   const [tutorialStarted, setTutorialStarted] = useState(false);\n//   const videoRef = useRef(null);\n//   const canvasRef = useRef(null);\n//   const [processing, setProcessing] = useState(false);\n//   const audioRef = useRef(new Audio('/distilled_water.mp3'));\n\n//   const startTutorial = () => {\n//     audioRef.current.play().then(() => {\n//       setTutorialStarted(true); // Start camera feed after audio plays\n//     }).catch(err => {\n//       console.error(\"Audio play error:\", err);\n//     });\n//   };\n\n//   useEffect(() => {\n//     if (tutorialStarted) {\n//       const video = videoRef.current;\n\n//       // Request access to the webcam\n//       navigator.mediaDevices.getUserMedia({ video: true })\n//         .then((stream) => {\n//           video.srcObject = stream;\n\n//           // Only call play() when the video is loaded\n//           video.onloadedmetadata = () => {\n//             video.play().catch(error => console.error(\"Error playing video:\", error));\n//           };\n//         })\n//         .catch(err => console.error(\"Error accessing webcam:\", err));\n\n//       return () => {\n//         if (video.srcObject) {\n//           video.srcObject.getTracks().forEach(track => track.stop()); // Stop webcam access when the component unmounts\n//         }\n//       };\n//     }\n//   }, [tutorialStarted]);\n\n//   let lastProcessedTime = Date.now();\n\n//   const processFrame = async () => {\n//     const currentTime = Date.now();\n//     const timeDiff = currentTime - lastProcessedTime;\n\n//     // Adjust frame rate by only sending frames every 200ms (5 FPS, adjust as necessary)\n//     if (timeDiff < 200) {\n//       return;\n//     }\n\n//     lastProcessedTime = currentTime;\n\n//     const canvas = canvasRef.current;\n//     const bufferCanvas = document.createElement('canvas');  // Create an off-screen buffer canvas\n//     bufferCanvas.width = canvas.width;\n//     bufferCanvas.height = canvas.height;\n\n//     const context = canvas.getContext('2d');\n//     const bufferContext = bufferCanvas.getContext('2d');  // Context for the off-screen canvas\n\n//     // Draw the current frame from video on buffer canvas\n//     bufferContext.drawImage(videoRef.current, 0, 0, bufferCanvas.width, bufferCanvas.height);\n\n//     // Convert the frame to Blob and send to the backend for processing\n//     bufferCanvas.toBlob(async (blob) => {\n//       const formData = new FormData();\n//       formData.append('image', blob, 'frame.jpg');\n\n//       try {\n//         setProcessing(true);\n//         const response = await axios.post('http://localhost:5000/detect', formData, {\n//           responseType: 'json'\n//         });\n\n//         // Handle the response\n//         const { detected, augmented_image } = response.data;\n\n//         // Create an image object from the base64 encoded image\n//         const img = new Image();\n//         img.src = `data:image/jpeg;base64,${augmented_image}`;\n//         img.onload = () => {\n//           const canvasContext = canvas.getContext('2d');\n//           canvasContext.drawImage(img, 0, 0, canvas.width, canvas.height);  // Draw the augmented frame\n//           setProcessing(false);\n//           setDetected(detected);  // Set detected based on the backend response\n//         };\n\n//       } catch (err) {\n//         console.error(\"Error processing frame:\", err);\n//         setProcessing(false);\n//         setDetected(false);  // Ensure detection state is false if an error occurs\n//       }\n//     }, 'image/jpeg');\n//   };\n\n//   useEffect(() => {\n//     if (tutorialStarted) {\n//       const interval = setInterval(processFrame, 1000 / 10); // Process 10 frames per second\n//       return () => clearInterval(interval);\n//     }\n//   }, [tutorialStarted]);\n\n//   return (\n//     <div>\n//       {!tutorialStarted && <button onClick={startTutorial}>Start Tutorial</button>}\n//       <video ref={videoRef} style={{ display: 'none' }} />\n//       <canvas ref={canvasRef} width=\"640\" height=\"480\" />\n//       {/* Display the text \"Chemical pouring\" only when marker with ID 0 is detected */}\n//       {/* {detected && <p style={{ color: 'green', marginTop: '10px' }}>Chemical pouring</p>} */}\n//       {/* Display \"Completed\" when marker with ID 0 is detected */}\n//       {detected && <p style={{ color: 'blue', marginTop: '10px' }}>Completed</p>}\n//     </div>\n//   );\n// };\n\n// export default CameraFeed;\n\n\n\n\n\n\n\n\n\n\n// import React, { useEffect, useRef, useState } from 'react';\n// import axios from 'axios';\n\n// const CameraFeed = () => {\n//   const [detected, setDetected] = useState(false);\n//   const [completed, setCompleted] = useState(false);\n//   const videoRef = useRef(null);\n//   const canvasRef = useRef(null);\n//   const [processing, setProcessing] = useState(false);\n//   const audioRef = useRef(new Audio('/voice1.mp3'));\n//   const completionAudioRef = useRef(new Audio('/completed.mp3'));\n\n//   const startTutorial = () => {\n//     audioRef.current.play().catch(err => console.error(\"Audio play error:\", err));\n//   };\n\n//   useEffect(() => {\n//     const video = videoRef.current;\n\n//     // Request access to the webcam\n//     navigator.mediaDevices.getUserMedia({ video: true })\n//       .then((stream) => {\n//         video.srcObject = stream;\n\n//         // Only call play() when the video is loaded\n//         video.onloadedmetadata = () => {\n//           video.play().catch(error => console.error(\"Error playing video:\", error));\n//         };\n//       })\n//       .catch(err => console.error(\"Error accessing webcam:\", err));\n\n//     return () => {\n//       if (video.srcObject) {\n//         video.srcObject.getTracks().forEach(track => track.stop()); // Stop webcam access when the component unmounts\n//       }\n//     };\n//   }, []);\n\n//   let lastProcessedTime = Date.now();\n\n//   const processFrame = async () => {\n//     if (completed) return; // Stop processing if completed\n  \n//     const currentTime = Date.now();\n//     const timeDiff = currentTime - lastProcessedTime;\n  \n//     // Adjust frame rate by only sending frames every 200ms (5 FPS, adjust as necessary)\n//     if (timeDiff < 200) {\n//       return;\n//     }\n  \n//     lastProcessedTime = currentTime;\n  \n//     const canvas = canvasRef.current;\n//     const bufferCanvas = document.createElement('canvas');  // Create an off-screen buffer canvas\n//     bufferCanvas.width = canvas.width;\n//     bufferCanvas.height = canvas.height;\n  \n//     const context = canvas.getContext('2d');\n//     const bufferContext = bufferCanvas.getContext('2d');  // Context for the off-screen canvas\n  \n//     // Draw the current frame from video on buffer canvas\n//     bufferContext.drawImage(videoRef.current, 0, 0, bufferCanvas.width, bufferCanvas.height);\n  \n//     // Convert the frame to Blob and send to the backend for processing\n//     bufferCanvas.toBlob(async (blob) => {\n//       const formData = new FormData();\n//       formData.append('image', blob, 'frame.jpg');\n  \n//       try {\n//         setProcessing(true);\n//         const response = await axios.post('http://localhost:5000/detect', formData, {\n//           responseType: 'json'\n//         });\n  \n//         console.log(\"Response Data:\", response.data); // Log the entire response data\n  \n//         // Handle the response\n//         const { detected, augmented_image, ids } = response.data;\n  \n//         // Check if detection happened\n//         if (detected) {\n//           console.log('Detection is true');\n//           console.log('Detected IDs:', ids); // This should now log the detected IDs\n        \n//           // Check if the detected marker is ID 0\n//           if (ids && ids.includes(0)) {\n//             setCompleted(true);\n//             console.log('Marker ID 0 detected');\n//             videoRef.current.srcObject.getTracks().forEach(track => track.stop()); // Pause camera stream\n//             completionAudioRef.current.play().catch(err => console.error(\"Completion audio play error:\", err));\n//           }\n//         }\n        \n  \n//         // Create an image object from the base64 encoded image\n//         const img = new Image();\n//         img.src = `data:image/jpeg;base64,${augmented_image}`;\n//         img.onload = () => {\n//           const canvasContext = canvas.getContext('2d');\n//           canvasContext.drawImage(img, 0, 0, canvas.width, canvas.height);  // Draw the augmented frame\n//           setProcessing(false);\n//           setDetected(detected);  // Set detected based on the backend response\n//         };\n  \n//       } catch (err) {\n//         console.error(\"Error processing frame:\", err);\n//         setProcessing(false);\n//         setDetected(false);  // Ensure detection state is false if an error occurs\n//       }\n//     }, 'image/jpeg');\n//   };\n\n//   useEffect(() => {\n//     const interval = setInterval(processFrame, 1000 / 10); // Process 10 frames per second\n//     return () => clearInterval(interval);\n//   }, [completed]);\n\n//   return (\n//     <div>\n//       <button onClick={startTutorial}>Start Tutorial</button>\n//       <video ref={videoRef} style={{ display: 'none' }} />\n//       <canvas ref={canvasRef} width=\"640\" height=\"480\" />\n//       {/* Display the text \"Chemical pouring\" only when marker is detected */}\n//       {/* {detected && <p style={{ color: 'green', marginTop: '10px' }}>Chemical pouring</p>} */}\n//       {completed && <p style={{ color: 'blue', marginTop: '10px' }}>Completed</p>}\n//     </div>\n//   );\n// };\n\n// export default CameraFeed;\n\n\n\n\n\n// import React, { useEffect, useRef, useState } from 'react';\n// import axios from 'axios';\n\n// const CameraFeed = () => {\n//   const [detected, setDetected] = useState(false);\n//   const [completed, setCompleted] = useState(false);\n//   const [currentStep, setCurrentStep] = useState(0);  // Track the current step\n//   const videoRef = useRef(null);\n//   const canvasRef = useRef(null);\n//   const audioRefs = useRef([\n//     new Audio('/voice1.mp3'), // Step 1: Introduction to sulfate test\n//     new Audio('/voice2.mp3'), // Step 2: Show distilled water\n//     new Audio('/voice3.mp3'), // Step 3: Show test tube 1\n//     // Add the rest of the steps here\n//     new Audio('/voice13.mp3'), // Step 13: Test completion message\n//   ]);\n\n//   useEffect(() => {\n//     // Play the first step's audio when the component is loaded\n//     audioRefs.current[0].play().catch(err => console.error(\"Audio play error:\", err));\n\n//     // Web camera initialization logic\n//     const video = videoRef.current;\n//     navigator.mediaDevices.getUserMedia({ video: true })\n//       .then((stream) => {\n//         video.srcObject = stream;\n//         video.onloadedmetadata = () => {\n//           video.play().catch(error => console.error(\"Error playing video:\", error));\n//         };\n//       })\n//       .catch(err => console.error(\"Error accessing webcam:\", err));\n\n//     return () => {\n//       if (video.srcObject) {\n//         video.srcObject.getTracks().forEach(track => track.stop());\n//       }\n//     };\n//   }, []);\n\n//   const processFrame = async () => {\n//     if (completed) return;  // Stop processing if test is complete\n\n//     const canvas = canvasRef.current;\n//     const context = canvas.getContext('2d');\n//     context.drawImage(videoRef.current, 0, 0, canvas.width, canvas.height);\n\n//     // Send frame to backend for ArUco detection\n//     canvas.toBlob(async (blob) => {\n//       const formData = new FormData();\n//       formData.append('image', blob, 'frame.jpg');\n//       try {\n//         const response = await axios.post('http://localhost:5000/detect', formData, { responseType: 'json' });\n//         const { detected, ids } = response.data;\n\n//         if (detected && ids) {\n//           handleMarkerDetection(ids);\n//         }\n\n//         // Display processed frame (augmented) coming from backend\n//         const img = new Image();\n//         img.src = `data:image/jpeg;base64,${response.data.augmented_image}`;\n//         img.onload = () => context.drawImage(img, 0, 0, canvas.width, canvas.height);\n//       } catch (err) {\n//         console.error(\"Error processing frame:\", err);\n//       }\n//     }, 'image/jpeg');\n//   };\n\n//   const handleMarkerDetection = (ids) => {\n//     const expectedMarkerId = currentStep;  // Assuming marker ID matches the current step\n//     if (ids.includes(expectedMarkerId)) {\n//       console.log(`Step ${currentStep} verified with marker ID: ${expectedMarkerId}`);\n\n//       // Move to the next step\n//       setCurrentStep(currentStep + 1);\n      \n//       // Stop current audio and play the next\n//       if (currentStep < audioRefs.current.length) {\n//         audioRefs.current[currentStep].pause();\n//         audioRefs.current[currentStep + 1].play().catch(err => console.error(\"Audio play error:\", err));\n//       }\n\n//       // Mark test as completed when last step is done\n//       if (currentStep === audioRefs.current.length - 1) {\n//         setCompleted(true);\n//       }\n//     }\n//   };\n\n//   useEffect(() => {\n//     const interval = setInterval(processFrame, 1000 / 10);  // 10 FPS\n//     return () => clearInterval(interval);\n//   }, [currentStep]);\n\n//   return (\n//     <div>\n//       <video ref={videoRef} style={{ display: 'none' }} />\n//       <canvas ref={canvasRef} width=\"640\" height=\"480\" />\n//       {completed && <p style={{ color: 'blue', marginTop: '10px' }}>Sulfate Ion Test Completed</p>}\n//     </div>\n//   );\n// };\n\n// export default CameraFeed;\n\n\n\n\n\nimport React, { useEffect, useRef, useState } from 'react';\nimport axios from 'axios';\n\nconst CameraFeed = () => {\n  const [detected, setDetected] = useState(false);\n  const [completed, setCompleted] = useState(false);\n  const [currentStep, setCurrentStep] = useState(0);  // Track the current step\n  const [isTutorialStarted, setIsTutorialStarted] = useState(false); // Track whether tutorial has started\n  const videoRef = useRef(null);\n  const canvasRef = useRef(null);\n  // const audioRefs = useRef([\n  //   new Audio('/voice1.mp3'), // Step 1: Introduction to sulfate test\n  //   new Audio('/voice2.mp3'), // Step 2: Show distilled water\n  //   new Audio('/voice3.mp3'), // Step 3: Show test tube 1\n  //   // new Audio('/voice4.mp3'),\n  //   // Add the rest of the steps here\n  //   new Audio('/voice13.mp3'), // Step 13: Test completion message\n  // ]);\n  const audioRefs = useRef([\n    ['/voice1.mp3'], // Step 1: Introduction to sulfate test\n    ['/voice2.mp3'], // Step 2: Show distilled water\n    ['/voice3.mp3'], // Step 3: Show test tube 1\n    // Add more audio arrays as needed\n    ['/voice13.mp3'], // Step 13: Test completion message\n  ]);\n\n  const playAudios = (audios, index = 0) => {\n    if (index < audios.length) {\n      const audio = new Audio(audios[index]);\n      audio.play().catch(err => console.error(\"Audio play error:\", err));\n      audio.onended = () => playAudios(audios, index + 1); // Play next audio when current one ends\n    }\n  };\n\n  // Start camera feed and process frames only when the tutorial is started\n  useEffect(() => {\n    if (isTutorialStarted) {\n      const video = videoRef.current;\n\n      // Web camera initialization logic\n      navigator.mediaDevices.getUserMedia({ video: true })\n        .then((stream) => {\n          video.srcObject = stream;\n          video.onloadedmetadata = () => {\n            video.play();\n          };\n        })\n        .catch(err => console.error(\"Error accessing webcam:\", err));\n\n      // Clean up: stop the camera stream when component unmounts\n      return () => {\n        if (video.srcObject) {\n          video.srcObject.getTracks().forEach(track => track.stop());\n        }\n      };\n    }\n  }, [isTutorialStarted]);  // Only run this effect when the tutorial starts\n\n  const processFrame = async () => {\n    if (!isTutorialStarted || completed) return;  // Stop processing if the tutorial hasn't started or test is complete\n  \n    const canvas = canvasRef.current;\n    const context = canvas.getContext('2d');\n    \n    // Ensure video is ready and draw the video feed onto the canvas\n    const video = videoRef.current;\n    if (video.readyState === video.HAVE_ENOUGH_DATA) {\n      context.drawImage(video, 0, 0, canvas.width, canvas.height);\n  \n      // Send frame to backend for ArUco detection\n      canvas.toBlob(async (blob) => {\n        const formData = new FormData();\n        formData.append('image', blob, 'frame.jpg');\n        try {\n          const response = await axios.post('http://localhost:5000/detect', formData, { responseType: 'json' });\n          console.log(\"Response from backend:\", response.data);  // Log response\n  \n          const { detected, ids, augmented_image } = response.data;  // Destructure response\n          console.log(`Detected: ${detected}, IDs: ${ids}, Augmented Image Length: ${augmented_image.length}`); // Log image length\n  \n          if (detected && ids) {\n            handleMarkerDetection(ids);\n          }\n  \n          // Clear the canvas before drawing the new image\n          context.clearRect(0, 0, canvas.width, canvas.height);\n  \n          // Display processed frame (augmented) coming from backend\n          const img = new Image();\n          img.src = `data:image/jpeg;base64,${augmented_image}`;\n          img.onload = () => {\n            context.drawImage(img, 0, 0, canvas.width, canvas.height);\n          };\n        } catch (err) {\n          console.error(\"Error processing frame:\", err);\n        }\n      }, 'image/jpeg');\n    }\n  };\n  \n\n  const handleMarkerDetection = (ids) => {\n    const expectedMarkerId = currentStep;  // Assuming marker ID matches the current step\n    if (ids.includes(expectedMarkerId)) {\n      console.log(`Step ${currentStep} verified with marker ID: ${expectedMarkerId}`);\n\n      // Move to the next step\n      setCurrentStep(currentStep + 1);\n      \n      // Stop current audio and play the next\n      if (currentStep < audioRefs.current.length) {\n        console.log('-----------------------------------', currentStep)\n        audioRefs.current[currentStep].pause();\n        audioRefs.current[currentStep + 1].play().catch(err => console.error(\"Audio play error:\", err));\n      }\n\n      // Mark test as completed when last step is done\n      if (currentStep === audioRefs.current.length - 1) {\n        setCompleted(true);\n      }\n    }\n  };\n\n  // Trigger the frame processing when the tutorial starts\n  useEffect(() => {\n    if (isTutorialStarted) {\n      const interval = setInterval(processFrame, 1000 / 10);  // 10 FPS\n      return () => clearInterval(interval);\n    }\n  }, [isTutorialStarted, currentStep]);\n\n  const startTutorial = () => {\n    setIsTutorialStarted(true);\n    // audioRefs.current[0].play().catch(err => console.error(\"Audio play error:\", err));\n    playAudios(audioRefs.current[step]); // Play audio for the specific step\n  };\n\n  // const startTutorial = () => {\n  //   setIsTutorialStarted(true);\n    \n  //   // Play the first audio\n  //   audioRefs.current[0].play().catch(err => console.error(\"Audio play error:\", err));\n  \n  //   // Set up an event listener to play the second audio after the first one ends\n  //   audioRefs.current[0].onended = () => {\n  //     audioRefs.current[1].play().catch(err => console.error(\"Audio play error:\", err));\n  //   };\n  // };\n  \n\n  return (\n    <div>\n      {!isTutorialStarted && (\n        <button onClick={startTutorial}>Start Tutorial</button>\n      )}\n      {isTutorialStarted && (\n        <div>\n          <video ref={videoRef} style={{ display: 'none' }} />\n          <canvas ref={canvasRef} width=\"640\" height=\"480\" style={{ border: '1px solid black' }} />\n          {completed && <p style={{ color: 'blue', marginTop: '10px' }}>Sulfate Ion Test Completed</p>}\n        </div>\n      )}\n    </div>\n  );\n};\n\nexport default CameraFeed;\n"],"mappings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vB,MAAM,CAACC,QAAQ,EAAEC,WAAW,CAAC,GAAGP,QAAQ,CAAC,KAAK,CAAC;EAC/C,MAAM,CAACQ,SAAS,EAAEC,YAAY,CAAC,GAAGT,QAAQ,CAAC,KAAK,CAAC;EACjD,MAAM,CAACU,WAAW,EAAEC,cAAc,CAAC,GAAGX,QAAQ,CAAC,CAAC,CAAC,CAAC,CAAE;EACpD,MAAM,CAACY,iBAAiB,EAAEC,oBAAoB,CAAC,GAAGb,QAAQ,CAAC,KAAK,CAAC,CAAC,CAAC;EACnE,MAAMc,QAAQ,GAAGf,MAAM,CAAC,IAAI,CAAC;EAC7B,MAAMgB,SAAS,GAAGhB,MAAM,CAAC,IAAI,CAAC;EAC9B;EACA;EACA;EACA;EACA;EACA;EACA;EACA;EACA,MAAMiB,SAAS,GAAGjB,MAAM,CAAC,CACvB,CAAC,aAAa,CAAC;EAAE;EACjB,CAAC,aAAa,CAAC;EAAE;EACjB,CAAC,aAAa,CAAC;EAAE;EACjB;EACA,CAAC,cAAc,CAAC,CAAE;EAAA,CACnB,CAAC;EAEF,MAAMkB,UAAU,GAAGA,CAACC,MAAM,EAAEC,KAAK,GAAG,CAAC,KAAK;IACxC,IAAIA,KAAK,GAAGD,MAAM,CAACE,MAAM,EAAE;MACzB,MAAMC,KAAK,GAAG,IAAIC,KAAK,CAACJ,MAAM,CAACC,KAAK,CAAC,CAAC;MACtCE,KAAK,CAACE,IAAI,CAAC,CAAC,CAACC,KAAK,CAACC,GAAG,IAAIC,OAAO,CAACC,KAAK,CAAC,mBAAmB,EAAEF,GAAG,CAAC,CAAC;MAClEJ,KAAK,CAACO,OAAO,GAAG,MAAMX,UAAU,CAACC,MAAM,EAAEC,KAAK,GAAG,CAAC,CAAC,CAAC,CAAC;IACvD;EACF,CAAC;;EAED;EACArB,SAAS,CAAC,MAAM;IACd,IAAIc,iBAAiB,EAAE;MACrB,MAAMiB,KAAK,GAAGf,QAAQ,CAACgB,OAAO;;MAE9B;MACAC,SAAS,CAACC,YAAY,CAACC,YAAY,CAAC;QAAEJ,KAAK,EAAE;MAAK,CAAC,CAAC,CACjDK,IAAI,CAAEC,MAAM,IAAK;QAChBN,KAAK,CAACO,SAAS,GAAGD,MAAM;QACxBN,KAAK,CAACQ,gBAAgB,GAAG,MAAM;UAC7BR,KAAK,CAACN,IAAI,CAAC,CAAC;QACd,CAAC;MACH,CAAC,CAAC,CACDC,KAAK,CAACC,GAAG,IAAIC,OAAO,CAACC,KAAK,CAAC,yBAAyB,EAAEF,GAAG,CAAC,CAAC;;MAE9D;MACA,OAAO,MAAM;QACX,IAAII,KAAK,CAACO,SAAS,EAAE;UACnBP,KAAK,CAACO,SAAS,CAACE,SAAS,CAAC,CAAC,CAACC,OAAO,CAACC,KAAK,IAAIA,KAAK,CAACC,IAAI,CAAC,CAAC,CAAC;QAC5D;MACF,CAAC;IACH;EACF,CAAC,EAAE,CAAC7B,iBAAiB,CAAC,CAAC,CAAC,CAAE;;EAE1B,MAAM8B,YAAY,GAAG,MAAAA,CAAA,KAAY;IAC/B,IAAI,CAAC9B,iBAAiB,IAAIJ,SAAS,EAAE,OAAO,CAAE;;IAE9C,MAAMmC,MAAM,GAAG5B,SAAS,CAACe,OAAO;IAChC,MAAMc,OAAO,GAAGD,MAAM,CAACE,UAAU,CAAC,IAAI,CAAC;;IAEvC;IACA,MAAMhB,KAAK,GAAGf,QAAQ,CAACgB,OAAO;IAC9B,IAAID,KAAK,CAACiB,UAAU,KAAKjB,KAAK,CAACkB,gBAAgB,EAAE;MAC/CH,OAAO,CAACI,SAAS,CAACnB,KAAK,EAAE,CAAC,EAAE,CAAC,EAAEc,MAAM,CAACM,KAAK,EAAEN,MAAM,CAACO,MAAM,CAAC;;MAE3D;MACAP,MAAM,CAACQ,MAAM,CAAC,MAAOC,IAAI,IAAK;QAC5B,MAAMC,QAAQ,GAAG,IAAIC,QAAQ,CAAC,CAAC;QAC/BD,QAAQ,CAACE,MAAM,CAAC,OAAO,EAAEH,IAAI,EAAE,WAAW,CAAC;QAC3C,IAAI;UACF,MAAMI,QAAQ,GAAG,MAAMvD,KAAK,CAACwD,IAAI,CAAC,8BAA8B,EAAEJ,QAAQ,EAAE;YAAEK,YAAY,EAAE;UAAO,CAAC,CAAC;UACrGhC,OAAO,CAACiC,GAAG,CAAC,wBAAwB,EAAEH,QAAQ,CAACI,IAAI,CAAC,CAAC,CAAE;;UAEvD,MAAM;YAAEtD,QAAQ;YAAEuD,GAAG;YAAEC;UAAgB,CAAC,GAAGN,QAAQ,CAACI,IAAI,CAAC,CAAE;UAC3DlC,OAAO,CAACiC,GAAG,CAAC,aAAarD,QAAQ,UAAUuD,GAAG,6BAA6BC,eAAe,CAAC1C,MAAM,EAAE,CAAC,CAAC,CAAC;;UAEtG,IAAId,QAAQ,IAAIuD,GAAG,EAAE;YACnBE,qBAAqB,CAACF,GAAG,CAAC;UAC5B;;UAEA;UACAjB,OAAO,CAACoB,SAAS,CAAC,CAAC,EAAE,CAAC,EAAErB,MAAM,CAACM,KAAK,EAAEN,MAAM,CAACO,MAAM,CAAC;;UAEpD;UACA,MAAMe,GAAG,GAAG,IAAIC,KAAK,CAAC,CAAC;UACvBD,GAAG,CAACE,GAAG,GAAG,0BAA0BL,eAAe,EAAE;UACrDG,GAAG,CAACG,MAAM,GAAG,MAAM;YACjBxB,OAAO,CAACI,SAAS,CAACiB,GAAG,EAAE,CAAC,EAAE,CAAC,EAAEtB,MAAM,CAACM,KAAK,EAAEN,MAAM,CAACO,MAAM,CAAC;UAC3D,CAAC;QACH,CAAC,CAAC,OAAOzB,GAAG,EAAE;UACZC,OAAO,CAACC,KAAK,CAAC,yBAAyB,EAAEF,GAAG,CAAC;QAC/C;MACF,CAAC,EAAE,YAAY,CAAC;IAClB;EACF,CAAC;EAGD,MAAMsC,qBAAqB,GAAIF,GAAG,IAAK;IACrC,MAAMQ,gBAAgB,GAAG3D,WAAW,CAAC,CAAE;IACvC,IAAImD,GAAG,CAACS,QAAQ,CAACD,gBAAgB,CAAC,EAAE;MAClC3C,OAAO,CAACiC,GAAG,CAAC,QAAQjD,WAAW,6BAA6B2D,gBAAgB,EAAE,CAAC;;MAE/E;MACA1D,cAAc,CAACD,WAAW,GAAG,CAAC,CAAC;;MAE/B;MACA,IAAIA,WAAW,GAAGM,SAAS,CAACc,OAAO,CAACV,MAAM,EAAE;QAC1CM,OAAO,CAACiC,GAAG,CAAC,qCAAqC,EAAEjD,WAAW,CAAC;QAC/DM,SAAS,CAACc,OAAO,CAACpB,WAAW,CAAC,CAAC6D,KAAK,CAAC,CAAC;QACtCvD,SAAS,CAACc,OAAO,CAACpB,WAAW,GAAG,CAAC,CAAC,CAACa,IAAI,CAAC,CAAC,CAACC,KAAK,CAACC,GAAG,IAAIC,OAAO,CAACC,KAAK,CAAC,mBAAmB,EAAEF,GAAG,CAAC,CAAC;MACjG;;MAEA;MACA,IAAIf,WAAW,KAAKM,SAAS,CAACc,OAAO,CAACV,MAAM,GAAG,CAAC,EAAE;QAChDX,YAAY,CAAC,IAAI,CAAC;MACpB;IACF;EACF,CAAC;;EAED;EACAX,SAAS,CAAC,MAAM;IACd,IAAIc,iBAAiB,EAAE;MACrB,MAAM4D,QAAQ,GAAGC,WAAW,CAAC/B,YAAY,EAAE,IAAI,GAAG,EAAE,CAAC,CAAC,CAAE;MACxD,OAAO,MAAMgC,aAAa,CAACF,QAAQ,CAAC;IACtC;EACF,CAAC,EAAE,CAAC5D,iBAAiB,EAAEF,WAAW,CAAC,CAAC;EAEpC,MAAMiE,aAAa,GAAGA,CAAA,KAAM;IAC1B9D,oBAAoB,CAAC,IAAI,CAAC;IAC1B;IACAI,UAAU,CAACD,SAAS,CAACc,OAAO,CAAC8C,IAAI,CAAC,CAAC,CAAC,CAAC;EACvC,CAAC;;EAED;EACA;;EAEA;EACA;;EAEA;EACA;EACA;EACA;EACA;;EAGA,oBACEzE,OAAA;IAAA0E,QAAA,GACG,CAACjE,iBAAiB,iBACjBT,OAAA;MAAQ2E,OAAO,EAAEH,aAAc;MAAAE,QAAA,EAAC;IAAc;MAAAE,QAAA,EAAAC,YAAA;MAAAC,UAAA;MAAAC,YAAA;IAAA,OAAQ,CACvD,EACAtE,iBAAiB,iBAChBT,OAAA;MAAA0E,QAAA,gBACE1E,OAAA;QAAOgF,GAAG,EAAErE,QAAS;QAACsE,KAAK,EAAE;UAAEC,OAAO,EAAE;QAAO;MAAE;QAAAN,QAAA,EAAAC,YAAA;QAAAC,UAAA;QAAAC,YAAA;MAAA,OAAE,CAAC,eACpD/E,OAAA;QAAQgF,GAAG,EAAEpE,SAAU;QAACkC,KAAK,EAAC,KAAK;QAACC,MAAM,EAAC,KAAK;QAACkC,KAAK,EAAE;UAAEE,MAAM,EAAE;QAAkB;MAAE;QAAAP,QAAA,EAAAC,YAAA;QAAAC,UAAA;QAAAC,YAAA;MAAA,OAAE,CAAC,EACxF1E,SAAS,iBAAIL,OAAA;QAAGiF,KAAK,EAAE;UAAEG,KAAK,EAAE,MAAM;UAAEC,SAAS,EAAE;QAAO,CAAE;QAAAX,QAAA,EAAC;MAA0B;QAAAE,QAAA,EAAAC,YAAA;QAAAC,UAAA;QAAAC,YAAA;MAAA,OAAG,CAAC;IAAA;MAAAH,QAAA,EAAAC,YAAA;MAAAC,UAAA;MAAAC,YAAA;IAAA,OACzF,CACN;EAAA;IAAAH,QAAA,EAAAC,YAAA;IAAAC,UAAA;IAAAC,YAAA;EAAA,OACE,CAAC;AAEV,CAAC;AAAC7E,EAAA,CAjKID,UAAU;AAAAqF,EAAA,GAAVrF,UAAU;AAmKhB,eAAeA,UAAU;AAAC,IAAAqF,EAAA;AAAAC,YAAA,CAAAD,EAAA","ignoreList":[]},"metadata":{},"sourceType":"module","externalDependencies":[]}